{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Booter Blacklist</h2> \n",
    "<h3>All in one</h3> \n",
    "\n",
    "<br>\n",
    "<div id=\"TOC\">\n",
    "<ul>\n",
    "\n",
    "<li><a href=\"#1\">1. Colecting URLs (and their information) </a></li>\n",
    "<ul>\n",
    "<li><a href=\"#1.1\">1.1. Scrutinizing URL(s)</a></li>\n",
    "<li><a href=\"#1.2\">1.2. Storing functions</a></li>\n",
    "<li><a href=\"#1.3\">1.3. Crawler</a></li>\n",
    "<li><a href=\"#1.4\">1.4. Scrape general informations from a Web page</a></li>\n",
    "</ul>\n",
    "\n",
    "<li><a href=\"#2\">2. Crawling Google </a></li>\n",
    "<li><a href=\"#3\">3. Crawling Youtube </a></li>\n",
    "<li><a href=\"#4\">4. Crawling Hackerforum.net </a></li>\n",
    "\n",
    "\n",
    "</ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.1\"><h2><a href=\"#TOC\">1.1. Scrutinizing URL(s)</a></h2></div>\n",
    "\n",
    "This class extracts several attributes of a URL (string input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('www.jairsantanna.com',\n",
       " '/pages/hame.index',\n",
       " '?test',\n",
       " 'http',\n",
       " 'http://www.jairsantanna.com/pages/hame.index',\n",
       " '/pages/hame.index',\n",
       " '?test',\n",
       " 'http://www.jairsantanna.com/pages/hame.index?test',\n",
       " '?',\n",
       " 'jairsantanna.com')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urlparse import urlparse\n",
    "\n",
    "# container format for potential Booter URL/domainname (PBD)\n",
    "# Holds hostname/domain, complete URL and easy access to other relevant data\n",
    "class BooterURL:\n",
    "    # constructor\n",
    "    def __init__(this, url):\n",
    "        # if url does not contain protocol; add it.\n",
    "        if 'http' not in url:\n",
    "            url = 'http://' + url\n",
    "        # parse URL and store relevant data\n",
    "        parsed  = urlparse(url)\n",
    "        this.Hostname = parsed.hostname\n",
    "        this.Scheme   = parsed.scheme\n",
    "        this.URL = parsed.scheme + '://' + parsed.netloc + parsed.path\n",
    "        this.Path     = parsed.path\n",
    "        this.Query    = '?' + parsed.query if parsed.query != '' else ''\n",
    "        this.Full_URL = url\n",
    "        this.Status\t  = '?'\n",
    "\n",
    "    # returns a URL representation that uniquely identifies the current URL \n",
    "    # this is the exact format described as a Potential Booter domain name (PBD)\n",
    "    # Type 2 URLs are omitted\n",
    "    def UniqueName(this):\n",
    "        # here we assume the hostname to be a unique identification \n",
    "        protocol = this.Scheme + '://' if this.Scheme else ''\n",
    "        if this.Hostname:\n",
    "            return this.Hostname.replace('www.','')\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def __str__(this):\n",
    "        return this.UniqueName()\n",
    "\n",
    "#################    \n",
    "# USAGE EXAMPLE #\n",
    "#################\n",
    "url_test = BooterURL('http://www.jairsantanna.com/pages/hame.index?test')\n",
    "url_test.Hostname,\\\n",
    "url_test.Path,\\\n",
    "url_test.Query,\\\n",
    "url_test.Scheme,\\\n",
    "url_test.URL,\\\n",
    "url_test.Path,\\\n",
    "url_test.Query,\\\n",
    "url_test.Full_URL,\\\n",
    "url_test.Status,\\\n",
    "url_test.UniqueName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "sqlite_file = 'BOOTERS_TRAINING.db'    # name of the sqlite database file\n",
    "table_name1 = 'urls'  # name of the table to be created\n",
    "new_field = 'my_1st_column' # name of the column\n",
    "field_type = 'INTEGER'  # column data type\n",
    "\n",
    "# Connecting to the database file\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Creating a new SQLite table with 1 column\n",
    "c.execute(\"CREATE TABLE urls ( domainName, booterURL TXT,last_update TXT, nr_pages TXT,url_type, average_depth_level, average_url_length, domain_age,domain_reservation_duration, whois_private, dps, page_rank, average_content_size,outbound_hyperlinks, category_specific_dictionary, resolver_indication, terms_of_services_page, login_form_depth_level)\")\n",
    "\n",
    "\n",
    "\n",
    "# Committing changes and closing the connection to the database file\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.2\"><h2><a href=\"#TOC\">Storing functions</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# storage.py hosts several utility database functions for storage\n",
    "# of Booter and crawl-related results. \n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# open connection and retrieve (single) cursor\n",
    "connection = sqlite3.connect('BOOTERS_TRAINING.db') \n",
    "\n",
    "# saves a Booter URL in the database.\n",
    "# if the Booter URL was not yet found a row is inserted,\n",
    "# otherwise updated.\n",
    "def SaveURL(booterURL, source = '', status='?', notes=''):\n",
    "    url = booterURL.URL\n",
    "    url_unique = booterURL.UniqueName()\n",
    "    # check if booter's url already exists\n",
    "    if RowExists('urls', url_unique):\n",
    "        # if entry exists, only do necessary updates\n",
    "        # Update('urls', url_unique, 'status', status) # disable for manual verification of training set\n",
    "        Update('urls', url_unique, 'timeUpdate', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')) # so we can see which URLS are from august 1 onwards (test set)\n",
    "        # also append source information if not yet stored\n",
    "        sources = GetSingleValue('urls', url_unique, 'srcInformation')\n",
    "        if source not in sources:\n",
    "            Update('urls', url_unique, 'srcInformation', source if sources == '' else sources + ';' + source)\n",
    "    else:\n",
    "        # else, insert into database\n",
    "        Insert('urls', [url_unique, url, 'CURRENT_DATE', '?', status, source, 'CURRENT_DATE', 'A', notes])\n",
    "\n",
    "# saves a score vector of a single Booter in the database\n",
    "# if the score vector was not yet found a row is inserted,\n",
    "# otherwise updated.\n",
    "def SaveScore(table, booterURL, \n",
    "    last_update, nr_pages, url_type, average_depth_level, average_url_length, domain_age, \n",
    "    domain_reservation_duration, whois_private, dps, page_rank, average_content_size, \n",
    "    outbound_hyperlinks, category_specific_dictionary, resolver_indication, terms_of_services_page,\n",
    "    login_form_depth_level):\n",
    "    url = booterURL.URL\n",
    "    url_unique = booterURL.UniqueName()\n",
    "    # check if booter's url already exists\n",
    "    if RowExists(table, url_unique):\n",
    "        # if entry exists, only do a necessary updates\n",
    "        Update(table, url_unique, 'lastUpdate', last_update) \n",
    "        Update(table, url_unique, 'nr_pages', nr_pages) \n",
    "        Update(table, url_unique, 'url_type', url_type) \n",
    "        Update(table, url_unique, 'average_depth_level', average_depth_level) \n",
    "        Update(table, url_unique, 'average_url_length', average_url_length) \n",
    "        Update(table, url_unique, 'domain_age', domain_age) \n",
    "        Update(table, url_unique, 'domain_reservation_duration', domain_reservation_duration) \n",
    "        Update(table, url_unique, 'whois_private', whois_private) \n",
    "        Update(table, url_unique, 'dps', dps) \n",
    "        Update(table, url_unique, 'page_rank', page_rank) \n",
    "        Update(table, url_unique, 'average_content_size', average_content_size) \n",
    "        Update(table, url_unique, 'outbound_hyperlinks', outbound_hyperlinks) \n",
    "        Update(table, url_unique, 'category_specific_dictionary', category_specific_dictionary) \n",
    "        Update(table, url_unique, 'resolver_indication', resolver_indication) \n",
    "        Update(table, url_unique, 'terms_of_services_page', terms_of_services_page) \n",
    "        Update(table, url_unique, 'login_form_depth_level', login_form_depth_level) \n",
    "    else:\n",
    "        # else, insert into database\n",
    "        Insert(table, [url_unique, last_update, nr_pages, url_type, average_depth_level,\n",
    "            average_url_length, domain_age, domain_reservation_duration, whois_private, dps,\n",
    "            page_rank, average_content_size, outbound_hyperlinks, category_specific_dictionary,\n",
    "            resolver_indication, terms_of_services_page, login_form_depth_level])\t\n",
    "\n",
    "# Checks whether a row/entry already exists by comparing a specific column with a check_value for uniqueness\n",
    "def RowExists(table, url):\n",
    "    query = 'SELECT domainName FROM ' + table + ' WHERE domainName = \\'' + url + '\\''\n",
    "    cursor = connection.execute(query)\n",
    "    if cursor.fetchall():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Returns a single value of a single column from the database\n",
    "def GetSingleValue(table, key_value, column, key_column = 'domainName'):\n",
    "    query = 'SELECT ' + column + ' FROM ' + table + ' WHERE ' + key_column + ' = \\'' + key_value + '\\''\n",
    "    cursor = connection.execute(query)\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "# For easy insert statements\n",
    "def Insert(table, values):\n",
    "    query = 'INSERT INTO ' + table + ' VALUES ('\n",
    "    for value in values:\n",
    "        if value == 'CURRENT_DATE':\n",
    "            query += value + ', '\n",
    "        else:\n",
    "            query += '\\'' + str(value) + '\\'' + ', '\n",
    "    query = query[:-2] + ')'\n",
    "    connection.execute(query)\n",
    "    connection.commit()\n",
    "\n",
    "# For easy update statements\n",
    "def Update(table, key, column, value):\n",
    "    query = 'UPDATE ' + table + ' SET ' + column + ' = \\'' + str(value) + '\\' WHERE domainName = \\'' + key + '\\''\n",
    "    connection.execute(query)\n",
    "    connection.commit()\n",
    "\n",
    "def Select(query):\n",
    "    result = []\n",
    "    for row in connection.execute(query):\n",
    "        result.append(row)\n",
    "    return result\n",
    "\n",
    "def CloseConnection():\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "# if explicitly called as root python file, do some debugging operations\n",
    "# if __name__ == \"__main__\":\n",
    "    # store in database\n",
    "#     url = BooterURL('http://booter.xyz/register.php')\n",
    "#     SaveURL(url, 'joeydevries.com', 'Y')\n",
    "#     url = BooterURL('http://www.booter.io')\n",
    "#     SaveURL(url, 'learnopengl.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import cfscrape\n",
    "import pythonwhois # https://github.com/joepie91/python-whois\n",
    "import signal\n",
    "from lxml import etree\n",
    "from colorama import Fore, Back, Style\n",
    "from random import choice, random\n",
    "from time import sleep\n",
    "from urlparse import urlparse\n",
    "\n",
    "\n",
    "# simple callback class for timer management\n",
    "class timeout:\n",
    "    def __init__(self, seconds=1, error_message='Timeout'):\n",
    "        self.seconds = seconds\n",
    "        self.error_message = error_message\n",
    "    def handle_timeout(self, signum, frame):\n",
    "        raise TimeoutError(self.error_message)\n",
    "    def __enter__(self):\n",
    "        signal.signal(signal.SIGALRM, self.handle_timeout)\n",
    "        signal.alarm(self.seconds)\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.alarm(0)\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    'General purpose Crawler; hosts functionality relevant to crawling a '\n",
    "    'multitude of online web applications like forums, video-platforms and '\n",
    "    'social media. The Crawler is not operable by itself, but acts as a '\n",
    "    'superclass for specific crawler instances per web application.'\n",
    "    def __init__(this, target, sleep_level=1):\n",
    "        this.Target = target\n",
    "        this.Sleep_Level = sleep_level\n",
    "        this.URLs = []\n",
    "\n",
    "        # a list of domains excluded from Booter identification\n",
    "        # these are frequently found in the results and can easily be excluded\n",
    "        # from all calculations.\n",
    "        # - can be extended\n",
    "        this.Excludes = {\n",
    "            'youtube.com',\n",
    "            'gyazo.com',\n",
    "            'dropbox.com',\n",
    "            'uploading.com',\n",
    "            'ge.tt',\n",
    "            'rapidshare.com',\n",
    "            'facebook.com',\n",
    "            'twitter.com',\n",
    "            'hackforums.net',\n",
    "            'imgur.net',\n",
    "            'imgur.com',\n",
    "            'mediafire.com',\n",
    "            'prntscr.com',\n",
    "            'pastebin.com',\n",
    "            'wikipedia',\n",
    "        }\n",
    "        # heuristic phrases to determine whether a website/domain is parked i.e. for sale\n",
    "        # - can be extended\n",
    "        this.ParkPhrases = {\n",
    "            \"this domain may be for sale\", \n",
    "            \"this domain is for sale\", \n",
    "            \"buy this domain\", \n",
    "            \"this web page is parked\", \n",
    "            \"this domain name expired on\", \n",
    "            \"backorder this domain\", \n",
    "            \"this domain is available through\",\n",
    "        }\n",
    "\n",
    "    # =========================================================================\n",
    "    # INITIALIZATION\n",
    "    # =========================================================================\n",
    "    # configures all connection objects and optionally logins into the service\n",
    "    def Initialize(this):\n",
    "        this.PrintUpdate('initiating crawling procedures')\n",
    "        this.PrintDivider()\n",
    "        this.Session = requests.Session()\t\t\n",
    "        # possible user-agent strings for the crawler system's 'user-agent' header flag\n",
    "        # a random user_agent string is selected each subsequent crawler run as to help\n",
    "        # avoid detection; manual update required from time to time\n",
    "        user_agents = [ \n",
    "            # 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',\n",
    "            # 'Opera/9.25 (Windows NT 5.1; U; en)',\n",
    "            # 'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "            # 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36', # this is currently enough for all purposes\n",
    "        ] \n",
    "        # http header\n",
    "        this.Header = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "            'Accept-Language' : 'nl-NL,nl;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Cache-Control' : 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'User-Agent': choice(user_agents),\n",
    "        }\n",
    "        this.PrintDebug(str(this.Header))\n",
    "\n",
    "\n",
    "    # follows login procedures to initiate a session object per web application\n",
    "    def Login(this, url, succes_page, post_data={}):\n",
    "        this.PrintDivider()\n",
    "        this.PrintUpdate('attempting to login at: ' + url)\n",
    "        this.PrintDivider()\n",
    "\n",
    "        response = this.Session.post(url, data=post_data, headers=this.Header)\n",
    "        js_scraper = cfscrape.create_scraper()\n",
    "        # login process specific to hackforums.net; during the reseach hackforums.net enabled additional\n",
    "        # security checks at their login services. Continously requesting login attemps seems to bypass\n",
    "        # their detective measures. Can (and should) be generlized in the future.\n",
    "        redirect_count = 1\n",
    "        while 'HackForums.net has enabled additional security.' in response.text: \n",
    "            print('HackForums.net detection bypass -redirect.')\n",
    "            response = this.Session.post(url, data=post_data, headers=this.Header)\n",
    "            redirect_count = redirect_count + 1\n",
    "            if redirect_count == 5000: # increase to 5000, seemed to work last time?\n",
    "                break\n",
    "        if response.url == succes_page:\n",
    "            this.PrintUpdate('login succesful')\n",
    "            this.PrintDivider()\n",
    "            return True\n",
    "        else:\n",
    "            this.PrintError('failed to login to target server. Is the target online or blocked?')\n",
    "            return False\n",
    "\n",
    "    # adds a list of url substrings thst should be excluded from list of URLs\n",
    "    def AddExcludes(this, excludes):\n",
    "        this.Excludes = excludes\n",
    "\n",
    "\n",
    "    # =========================================================================\n",
    "    # CRAWLER: GENERATING A PBD LIST\n",
    "    # =========================================================================\n",
    "    # crawls the web service, should be overriden in each subclass\n",
    "    def Crawl(this, max_date):\n",
    "        this.PrintError('Crawler.Crawl function not instantiated!')\n",
    "\n",
    "    # determines whether the url should be excluded\n",
    "    def IsExcluded(this, URL):\n",
    "        for excluded in this.Excludes:\n",
    "            if excluded in URL.Full_URL:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # determines whether a url is already added to the URL list\n",
    "    def IsDuplicate(this, URL):\n",
    "        for i in range(len(this.URLs)):\n",
    "            if this.URLs[i].UniqueName() == URL.UniqueName():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # adds a URL to the final URL/PBD list if it meets all conditions\n",
    "    def AddToList(this, URL, source='?'):\n",
    "        if not this.IsDuplicate(URL) and not this.IsExcluded(URL):\n",
    "            try:\n",
    "                # get status/respnse-code and resolved-url of URL\n",
    "                status = this.GetStatus(URL.Full_URL)\n",
    "                URL    = BooterURL(status[2])\n",
    "                # save in database\n",
    "                SaveURL(URL, source, status[0])\n",
    "                # then save in list if proper response or post error\n",
    "                if status[1] == 200 or status[1] == 403 or status[1] == 202:\n",
    "                    BooterURL.Status = status # add status to URL for later use\n",
    "                    this.URLs.append(URL)\n",
    "                    this.PrintLine('CRAWLER: ' + URL.Full_URL, Fore.BLUE)\n",
    "                    if status[1] == 403 or status[1] == 202:\n",
    "                        print('Website blocked crawler; manually verify!')\n",
    "                else:\n",
    "                    this.PrintNote('incorrect response code [' + str(status[1]) + ']: ' + URL.Full_URL)\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: ' + str(ex))\n",
    "\n",
    "\n",
    "\n",
    "    # finish crawling and output URL/PBD list to file\n",
    "    def Finish(this, output_file):\n",
    "        this.PrintUpdate('writing output to file \\'' + output_file + '\\'')\n",
    "        f = open(output_file, 'w')\n",
    "        for URL in this.URLs:\n",
    "            f.write(URL.UniqueName() + '\\n')\n",
    "        this.PrintUpdate('FINISHED; closing operations')\n",
    "        this.PrintDivider()\n",
    "        return this.URLs\n",
    "\n",
    "    # sleeps for a semi-random amount of time to mitigate bot detection\n",
    "    def Sleep(this):\n",
    "        # semi-randomly vary sleep amount to emulate human behavior\n",
    "        sleep(this.Sleep_Level + random() * this.Sleep_Level) \n",
    "\n",
    "    # enables javascript to circumvent JS bot detection\n",
    "    def JSCrawl(this, url):\n",
    "        js_scraper = cfscrape.create_scraper()\n",
    "        response = js_scraper.get(url, headers=this.Header, timeout=15.0) \n",
    "        redirect_count = 0\n",
    "        while 'http-equiv=\"Refresh\"' in response.text: \n",
    "            response = js_scraper.get(response.url, headers=this.Header, timeout=15.0) \n",
    "            redirect_count = redirect_count + 1\n",
    "            if redirect_count == 5:\n",
    "                this.PrintError('REDIRECT COUNT OF 5 REACHED! ' + response.url)\n",
    "                break\n",
    "        return response\n",
    "\n",
    "    # retrieves status of website: resolves URL, response code and whether it\n",
    "    # is offline/online or for sale\n",
    "    def GetStatus(this, url):\n",
    "        response = this.JSCrawl(url)\n",
    "        if response.status_code == 200 or response.status_code == 403 or response.status_code == 202:\n",
    "            # check if for sale, otherwise site deemed as online\n",
    "            for phrase in this.ParkPhrases:\n",
    "                if phrase in response.text.lower():\n",
    "                    return ('free', response.status_code, response.url)\n",
    "            # else site is online\n",
    "            return ('on', response.status_code, response.url)\t\t\t\t\n",
    "        else:\n",
    "            return ('off', response.status_code, response.url)\n",
    "\n",
    "    # =========================================================================\n",
    "    # SCRAPER: EVIDENCE AND HEURISTICS\n",
    "    # =========================================================================\n",
    "    # scrapes a (potential) Booter URL for evidence as reported by Booter \n",
    "    # characteristics in 'The Generation of Booter (black)lists'\n",
    "    def Scrape(this, URL, days_update=0):\n",
    "        # check if number of days_update days have passed since last update, and if so, update\n",
    "        if RowExists('scores', URL.UniqueName()): \n",
    "            last_update  = GetSingleValue('scores', URL.UniqueName(), 'lastUpdate')\n",
    "            last_update  = datetime.datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')\n",
    "            current_date = datetime.datetime.now()\n",
    "            difference   = (current_date - last_update).days\n",
    "            if difference < days_update:\n",
    "                this.PrintDivider()\n",
    "                this.PrintDebug('Skip scrape: ' + URL.UniqueName() + '; last scraped: ' + str(last_update))\n",
    "                return \n",
    "        # else, start scraping\n",
    "        try:\n",
    "            this.PrintDivider()\n",
    "            this.PrintDebug('STARTING SCRAPE: ' + URL.Full_URL)\n",
    "            ### 1. structure characteristics\n",
    "            this.PrintDivider()\n",
    "            this.PrintUpdate('obtaining structure-based characteristics')\n",
    "            this.PrintDivider()\n",
    "            # - 1.1 number of pages\n",
    "            number_of_pages \t= -1.0\n",
    "            number_of_pages_raw = -1.0 # also store a raw score for data analysis \n",
    "            # crawl through the URL and each subsequent inbound URL \n",
    "            crawled     = []\n",
    "            crawl_count = 0\n",
    "            max_urls    = 50\n",
    "            # - get landing page\n",
    "            landing_page = CrawlPage(this.JSCrawl(URL.Full_URL))\n",
    "            inbounds     = landing_page.GetInboundURLs()\n",
    "            crash_pages  = []\n",
    "\n",
    "            this.PrintNote('scraping: ' + URL.Full_URL)\t\n",
    "            page_url\t = (landing_page.URL.Hostname + landing_page.URL.Path).replace('//','/')\n",
    "            if page_url[len(page_url) - 1] == '/':\n",
    "                page_url = page_url[:-1]\n",
    "            depth_levels = { page_url : 0 }\n",
    "            inbounds.append(page_url)\n",
    "\n",
    "            # 1.1.1 store depth levels of landing page found inbound urls\n",
    "            for inbound in inbounds:\n",
    "                if inbound not in URL.Full_URL and inbound not in page_url:\n",
    "                    depth_levels[inbound] = 1\n",
    "\n",
    "            crawled.append(landing_page)\n",
    "\n",
    "            # 1.1.2. then from each found (inbound) URL, keep crawling until maximum crawl limit is reached\n",
    "            fail_loop_attempts = 0\t\t\n",
    "            while crawl_count < len(inbounds) and crawl_count < max_urls - 1:\n",
    "                inbound = inbounds[crawl_count]\n",
    "                if inbound not in crawled and inbound not in crash_pages:\n",
    "                    try:\n",
    "                        with timeout(seconds=20):\n",
    "                            # crawl next page and obtain new inbound/outbound urls\n",
    "                            this.PrintNote('scraping: ' + 'http://' + inbound)\n",
    "\n",
    "                            response = this.JSCrawl('http://' + inbound)\n",
    "                            crawl_page    = CrawlPage(response)\n",
    "                            new_inbounds  = crawl_page.GetInboundURLs()\n",
    "                            # for each of the new_inbounds, set their depth level if less than currently stored (or not yet stored)\n",
    "                            for new_inbound in new_inbounds:\n",
    "                                if new_inbound not in depth_levels:\n",
    "                                    depth_levels[new_inbound] = depth_levels[inbound] + 1\n",
    "                            # merge results\n",
    "                            inbounds  = inbounds  + list(set(new_inbounds)  - set(inbounds))\n",
    "                            # then continue\n",
    "                            crawled.append(crawl_page)\n",
    "                            crawl_count = crawl_count + 1\n",
    "                    except Exception as ex:\n",
    "                        this.PrintError('EXCEPTION: ' + str(ex))\n",
    "                        crash_pages.append(inbound)\n",
    "                        crawl_count = crawl_count + 1\n",
    "                else:\n",
    "                    fail_loop_attempts += 1 # aborts loop after 10000 tries; indicating infinite loop\n",
    "                    if fail_loop_attempts > 10000:\n",
    "                        this.PrintError('INFINITE LOOP detected; aborting scrape')\n",
    "                        break\n",
    "\n",
    "\n",
    "            # 1.1.3 calculate scores\n",
    "            # use quadratic equation (to give numbers with low pages higher scores)\n",
    "            # equation: y = -2x^2 + 1 ... (y=0) = 0.707\n",
    "            # with 25 pages, score is 0.75, so first half pages give 1/4 drop-down in score\n",
    "            number_of_pages_raw = len(inbounds)\n",
    "            if number_of_pages_raw > 0:\n",
    "                number_of_pages = -2 * (number_of_pages_raw / (50/0.707106781\t)) ** 2 + 1\n",
    "                number_of_pages = max(number_of_pages, 0.0) \n",
    "            this.PrintUpdate('number of pages: ' + str(number_of_pages_raw))\n",
    "\n",
    "            # - 1.2. URL type\n",
    "            # how to determine its url type? difficult/impossible to determine programmaticaly \t\n",
    "            url_type_raw = landing_page.URLType()\n",
    "            if url_type_raw == 2:\n",
    "                url_type = 0.0\n",
    "            else:\n",
    "                url_type = 1.0\n",
    "            this.PrintUpdate('url type: ' + str(url_type_raw))\n",
    "\n",
    "            # - 1.3. Average depth level\n",
    "            # take previously retrieved depth levels and take average\n",
    "            average_depth_level     = 0.0\n",
    "            average_depth_level_raw = 0.0\n",
    "            for depth_url in depth_levels:\n",
    "                average_depth_level_raw = average_depth_level_raw + depth_levels[depth_url]\n",
    "            # calculate score: take linear value between 1.0 and 3.0\n",
    "            average_depth_level_raw = average_depth_level_raw / len(depth_levels)\n",
    "            if average_depth_level_raw <= 1.0:\n",
    "                average_depth_level = 1.0\n",
    "            else:\n",
    "                average_depth_level = max(1.0 - ((average_depth_level_raw - 1.0) / 2.0), 0.0)\n",
    "            this.PrintUpdate('average depth level: ' + str(average_depth_level_raw))\n",
    "\n",
    "            # - 1.4. Average URL length\n",
    "            average_url_length     = -1.0\n",
    "            average_url_length_raw = -1.0\n",
    "            for page in inbounds: # use inbounds, not pages crawled as they give much more results\n",
    "                average_url_length_raw = average_url_length_raw + len(page)\n",
    "            # calculate score: interpolate linearly from lowest occurence to highest Booter occurence\n",
    "            average_url_length_raw = average_url_length_raw / len(inbounds) \n",
    "            if average_url_length_raw <= 15:\n",
    "                average_url_length = 1.0\n",
    "            else:\n",
    "                average_url_length = max(1.0 - ((average_url_length_raw - 15) / 15), 0.0)\n",
    "            this.PrintUpdate('average url length: ' + str(average_url_length_raw))\n",
    "\n",
    "\n",
    "            ### 2. content-based characteristics\n",
    "            this.PrintDivider()\n",
    "            this.PrintUpdate('obtaining content-based characteristics')\n",
    "            this.PrintDivider()\n",
    "\n",
    "            # get whois information\n",
    "            # \"Each part represents the response from a specific WHOIS server. Because the WHOIS doesn't force WHOIS \n",
    "            # servers to follow a unique response layout, each server needs its own dedicated parser.\"\n",
    "            domain_age                      = -1.0\n",
    "            domain_age_raw                  = -1.0\n",
    "            domain_reservation_duration     = -1.0\n",
    "            domain_reservation_duration_raw = -1.0\n",
    "            try:\n",
    "                with timeout(seconds=10):\n",
    "                    whois = pythonwhois.get_whois(landing_page.GetTopDomain(), False) # http://cryto.net/pythonwhois/usage.html\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: get WHOIS data: ' + str(ex))\n",
    "            try:\n",
    "                # - 2.1. Domain age\n",
    "                current_date    = datetime.datetime.today()\n",
    "                date_registered = whois['creation_date'][0]\n",
    "                domain_age_raw\t= (current_date - date_registered).days\n",
    "                # calculate score: linear interpolation between current_date and first occurence of \n",
    "                # booter in data: 2011\n",
    "                days_since_first = (current_date - datetime.datetime(2011, 10, 28)).days\n",
    "                domain_age = max(1.0 - (domain_age_raw / days_since_first), 0.0)\n",
    "                this.PrintUpdate('domain age: ' + str(domain_age_raw))\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: whois keywords, likely registrar: ' + str(ex))\n",
    "\n",
    "            try:\n",
    "                # - 2.2 Domain reservation duration\n",
    "                current_date  \t\t\t\t    = datetime.datetime.today()\n",
    "                expire_date    \t\t\t        = whois['expiration_date'][0]\n",
    "                domain_reservation_duration_raw = (expire_date - current_date).days\n",
    "                # calculate score: between 1 - 2 years; < 1 year = 1.0\n",
    "                if domain_reservation_duration_raw < 183:\n",
    "                    domain_reservation_duration = 1.0\n",
    "                else:\n",
    "                    # domain_reservation_duration = max(1.0 - ((domain_reservation_duration_raw - 365) / 365), 0.0)\n",
    "                    domain_reservation_duration = max(1.0 - (domain_reservation_duration_raw - 183) / 182, 0.0)\n",
    "                this.PrintUpdate('domain reservation duration: ' + str(domain_reservation_duration_raw))\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: whois keywords, likely registrar: ' + str(ex))\n",
    "\n",
    "            # - 2.3. WHOIS private\n",
    "            # there doesn't exist a private WHOIS field, but private information can be obtained through\n",
    "            # heuristics using common phrases found by privacy-replacing registry information.\n",
    "            try:\n",
    "                private_phrases = [\n",
    "                    'whoisguard',\n",
    "                    'whoisprotect',\n",
    "                    'domainsbyproxy',\n",
    "                    # 'whoisprivacyprotect', # are caught by privacy term anyways\n",
    "                    'protecteddomainservices',\n",
    "                    # 'myprivacy',\n",
    "                    # 'whoisprivacycorp',\n",
    "                    # 'privacyprotect',\n",
    "                    'namecheap',\n",
    "                    'privacy',\n",
    "                    'private',\n",
    "                ]\n",
    "                whois_private = 0.0\n",
    "                reg_name \t  = whois['contacts']['registrant']['name'].lower()\n",
    "                reg_email \t  = whois['contacts']['registrant']['email'].lower()\n",
    "                for phrase in private_phrases:\n",
    "                    if phrase in reg_name or phrase in reg_email: #or phrase in reg_org :\n",
    "                        whois_private = 1.0\n",
    "                        break\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: whois keyfields, private set to -1.0: ' + str(ex))\n",
    "                whois_private = -1.0\n",
    "            this.PrintUpdate('WHOIS private: ' + str(whois_private))\n",
    "\n",
    "            # - 2.4. DPS\n",
    "            # similar to whois private, use heuristics to determine whether website uses DPS,\n",
    "            # first we try to determine whether it uses DNS based DPS by checking nameservers\n",
    "            try:\n",
    "                with timeout(seconds=10):\n",
    "                    dps_names = [\n",
    "                        'cloudflare',\n",
    "                        'incapsula',\n",
    "                        'prolexic',\n",
    "                        'akamai',\n",
    "                        'verisign',\n",
    "                        'blazingfast',\n",
    "                    ]\n",
    "                    dps = 0.0\n",
    "                    if 'nameservers' in whois:\n",
    "                        for nameserver in whois['nameservers']:\n",
    "                            if dps == 0.0:\n",
    "                                for dps_name in dps_names:\n",
    "                                    if dps_name in nameserver.lower():\n",
    "                                        dps = 1.0\n",
    "                                        break\n",
    "                    # if nothing found from nameservers, also check redirection history if dps redirect page was used\n",
    "                    if dps < 0.5:\n",
    "                        response_text = this.Session.post(URL.Full_URL, headers=this.Header, allow_redirects=False).text\n",
    "                        this.PrintNote('No DPS detected from NS; checking re-direction history')\n",
    "                        for dps_name in dps_names:\n",
    "                            if dps_name in response_text:\n",
    "                                dps = 1.0\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: dps set to -1.0: ' + str(ex))\n",
    "                dps = -1.0\n",
    "            this.PrintUpdate('DPS: ' + str(dps))\n",
    "\n",
    "            # - 2.5. Page rank\n",
    "            try:\n",
    "                url           = 'http://data.alexa.com/data?cli=10&dat=s&url=' + URL.Hostname\n",
    "                response      = this.Session.get(url)\n",
    "                tree \t      = etree.XML(response.text.encode('utf-8'))\n",
    "                page_rank_raw = tree.xpath('(//REACH)/@RANK')[0]\n",
    "                page_rank     = 0.0\n",
    "                if int(page_rank_raw) > 200000: # determined from highest booter (ipstresser.com - vdos-s.com) minus offset\n",
    "                    page_rank = 1.0\n",
    "            except Exception as ex:\n",
    "                page_rank_raw = 25426978.0 # set to highest occuring page rank (lower than that if non-existent)\n",
    "                page_rank = 1.0\n",
    "\n",
    "            this.PrintUpdate('Page rank: ' + str(page_rank_raw))\n",
    "\n",
    "\n",
    "            ### 3. host-based characteristics\n",
    "            this.PrintDivider()\n",
    "            this.PrintUpdate('obtaining host-based characteristics')\n",
    "            this.PrintDivider()\n",
    "\n",
    "            # - 3.1. Average content size\n",
    "            average_content_size     = 0.0\n",
    "            average_content_size_raw = 0.0\n",
    "            crawl_contents = []\n",
    "            for crawl_page in crawled:\n",
    "                crawl_content = crawl_page.GetContent();\n",
    "                crawl_contents.append(crawl_content)\n",
    "                average_content_size_raw = average_content_size_raw + len(crawl_content)\n",
    "            average_content_size_raw = average_content_size_raw / len(crawled)\n",
    "            # calculte score: linear interpolation between 50 - (avg_max_booter = 250)\n",
    "            if average_content_size_raw < 50:\n",
    "                average_content_size = 1.0\n",
    "            else:\n",
    "                average_content_size = max(1.0 - (average_content_size_raw - 50) / 200, 0.0)\n",
    "            this.PrintUpdate('Average content size: ' + str(average_content_size_raw))\n",
    "\n",
    "            # - 3.2. Outbound hyperlinks\n",
    "            outbound_hyperlinks     = 0.0\n",
    "            outbound_hyperlinks_raw = 0.0\n",
    "            for crawl_page in crawled:\n",
    "                outbound_hyperlinks_raw = outbound_hyperlinks_raw + len(crawl_page.GetOutboundURLs())\n",
    "            outbound_hyperlinks_raw = outbound_hyperlinks_raw / len(crawled)\n",
    "            # calculate score: linear interpolation between 0 and 2\n",
    "            outbound_hyperlinks = max(1.0 - outbound_hyperlinks_raw / 2.0, 0.0)\n",
    "            this.PrintUpdate('Average outbound hyperlinks: ' + str(outbound_hyperlinks_raw))\n",
    "\n",
    "            # - 3.3. Category-specific dictionary\n",
    "            dictionary = [ 'stress', 'booter', 'ddos', 'powerful', 'resolver', 'price' ] # or pric, so we can also get items like pricing\n",
    "            category_specific_dictionary     = 0.0\n",
    "            category_specific_dictionary_raw = 0.0\n",
    "            words = landing_page.GetContent()\n",
    "            for item in dictionary:\n",
    "                for word in words:\n",
    "                    if item in word.lower():\n",
    "                        category_specific_dictionary_raw = category_specific_dictionary_raw + 1\n",
    "            # - now calculate percentage of these words occuring relative to total page content\n",
    "            if len(words) > 0:\n",
    "                category_specific_dictionary_raw = category_specific_dictionary_raw / len(words)\n",
    "            else:\n",
    "                category_specific_dictionary_raw = 0.0\n",
    "            # calculate score: interpolate between 0.01 and 0.05             \n",
    "            category_specific_dictionary = max(1.0 - ((category_specific_dictionary_raw - 0.01) / 0.04), 0.0)\n",
    "            this.PrintUpdate('Category specific dictionary: ' + str(category_specific_dictionary_raw))\n",
    "\n",
    "            # - 3.4. Resolver indication (only the landing page); perhaps extend to all pages in future version?\n",
    "            resolver_indication = 0.0\n",
    "            dictionary = [ 'skype' , 'xbox', 'resolve', 'cloudflare' ]\n",
    "            for item in dictionary:\n",
    "                for word in words:\n",
    "                    if item in word.lower():\n",
    "                        resolver_indication = 1.0\n",
    "            this.PrintUpdate('Resolver indication: ' + str(resolver_indication))\n",
    "\n",
    "            # - 3.5. Terms of Services page\n",
    "            terms_of_services_page = 0.0\n",
    "            # - check if one of the urls contains tos or terms and service\n",
    "            for url in inbounds:\n",
    "                url = url.lower();\n",
    "                if '/tos' in url or 'terms' in url and 'service' in url:\n",
    "                    terms_of_services_page = 1.0\n",
    "            # - if not yet found, also check for content hints in all the pages\n",
    "            tos_phrases = [\n",
    "                'terms and conditions', \n",
    "                'purposes intended', \n",
    "                'you are responsible', \n",
    "                'we have the right', \n",
    "                'terms of service',\n",
    "                'understand and agree',\n",
    "            ]\n",
    "            if terms_of_services_page < 0.5:\n",
    "                for content in crawl_contents:\n",
    "                    text = ' '.join(content).lower()\n",
    "                    for phrase in tos_phrases:\n",
    "                        if phrase in text:\n",
    "                            terms_of_services_page = 1.0\n",
    "                            break\n",
    "                    if terms_of_services_page > 0.5:\n",
    "                        break\n",
    "            this.PrintUpdate('Terms of services page: ' + str(terms_of_services_page))\n",
    "\n",
    "            # - 3.6. Login-form depth level\n",
    "            # this does also take into account register forms, but that's generally expected\n",
    "            # to be on the same level as login forms so not an issue\n",
    "            login_form_depth_level     = -1.0\n",
    "            login_form_depth_level_raw =  3.0 # set to max found in dataset if non-existent\n",
    "            forms_urls = []\n",
    "            for page in crawled:\n",
    "                if page.HasLoginForm(): \n",
    "                    page_url = page.URL.Hostname + page.URL.Path + page.URL.Query\n",
    "                    if page_url[len(page_url) - 1] == '/':\n",
    "                        page_url = page_url[:-1]\n",
    "                    forms_urls.append(page_url)\n",
    "            min_depth = 100\n",
    "            for url in forms_urls:\n",
    "                for depth_url in depth_levels:\n",
    "                    if depth_url == url:\n",
    "                        if depth_levels[url] < min_depth:\n",
    "                            min_depth = depth_levels[url]\n",
    "                        break\n",
    "            if min_depth != 100:\n",
    "                login_form_depth_level_raw = min_depth\n",
    "            # transform to score (if depth level exceeds 2, score becomes 0)\n",
    "            login_form_depth_level = min(max(1.0 - min_depth * 0.5, 0.0), 1.0)\n",
    "            this.PrintUpdate('Login-form depth level: ' + str(login_form_depth_level_raw))\n",
    "\n",
    "            ### 4. Now save the results into the database\n",
    "            SaveScore('scores',\n",
    "                URL,\n",
    "                datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                number_of_pages,\n",
    "                url_type,\n",
    "                average_depth_level,\n",
    "                average_url_length,\n",
    "                domain_age,\n",
    "                domain_reservation_duration,\n",
    "                whois_private,\n",
    "                dps,\n",
    "                page_rank,\n",
    "                average_content_size,\n",
    "                outbound_hyperlinks,\n",
    "                category_specific_dictionary,\n",
    "                resolver_indication,\n",
    "                terms_of_services_page,\n",
    "                login_form_depth_level\n",
    "            )\n",
    "            # also store raw feature data for analysis\n",
    "            SaveScore('characteristics',\n",
    "                URL,\n",
    "                datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                number_of_pages_raw,\n",
    "                url_type_raw,\n",
    "                average_depth_level_raw,\n",
    "                average_url_length_raw,\n",
    "                domain_age_raw,\n",
    "                domain_reservation_duration_raw,\n",
    "                whois_private,\n",
    "                dps,\n",
    "                page_rank_raw,\n",
    "                average_content_size_raw,\n",
    "                outbound_hyperlinks_raw,\n",
    "                category_specific_dictionary_raw,\n",
    "                resolver_indication,\n",
    "                terms_of_services_page,\n",
    "                login_form_depth_level_raw\n",
    "            )\n",
    "            this.Sleep()\n",
    "\n",
    "        except Exception as ex:\n",
    "            this.PrintError('EXCEPTION: Scrape failed: connection host ' + str(ex))\n",
    "            # raise\n",
    "\n",
    "\n",
    "\n",
    "    # =========================================================================\n",
    "    # PRINT FUNCTIONALITY\n",
    "    # =========================================================================\n",
    "    def PrintLine(this, text, color=Fore.WHITE, style=Style.NORMAL):\n",
    "        print(color + style + text[:80] + Style.RESET_ALL)\n",
    "    def PrintDivider(this):\n",
    "        this.PrintLine('................................................................................', Fore.YELLOW)\n",
    "    def PrintUpdate(this, text):\n",
    "        this.PrintLine('CRAWLER: ' + text, Fore.GREEN)\n",
    "    def PrintError(this, text):\n",
    "        this.PrintLine('CRAWLER: ' + text, Fore.RED)\n",
    "    def PrintDebug(this, text):\n",
    "        this.PrintLine('CRAWLER: ' + text, Fore.CYAN)\n",
    "    def PrintNote(this, text):\n",
    "        this.PrintLine('CRAWLER: ' + text, Fore.WHITE, Style.DIM)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape general informations from a Web page  [Crawl page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import cfscrape\n",
    "import tldextract # https://github.com/john-kurkowski/tldextract\n",
    "from lxml import html\n",
    "\n",
    "# hosts functionality and data relevant to crawling a single web page of a domain. \n",
    "# This includes per-page properties like html content, headers and functionality to retrieve all \n",
    "# inbound and/or outbound URLs and per-category dictionary queries.\n",
    "class CrawlPage:\n",
    "    # constructor\n",
    "    def __init__(this, response):\n",
    "        this.URL     = BooterURL(response.url)\n",
    "        this.HTML \t = response.text\n",
    "        this.Tree\t = html.fromstring(this.HTML)\n",
    "\n",
    "        # create relative path for further URL queries \n",
    "        path = this.URL.Path\n",
    "        if len(path) > 0 and path[0] =='/':\n",
    "            this.RelativeURL = (this.URL.Hostname).replace('//', '/')\t\t\t\n",
    "        else:\n",
    "            for i in reversed(path):\n",
    "                if i == '/':\n",
    "                    break\n",
    "                else:\n",
    "                    path = path[:-1]\n",
    "            this.RelativeURL = (this.URL.Hostname + path).replace('//', '/')\n",
    "        # print('relative:' + this.RelativeURL)\n",
    "\n",
    "        # add a 'contains' list of URLs NOT to scrape\n",
    "        this.Excludes = {\n",
    "            '#',\n",
    "            'mailto',\n",
    "            '.pdf',\n",
    "            '.doc',\n",
    "            '.rar',\n",
    "            '.zip',\t\t\n",
    "            '.png',\n",
    "            '.jpeg',\n",
    "            '.jpg',\n",
    "            '.gif',\n",
    "            '.bmp',\n",
    "            '.atom',\n",
    "            '.rss',\t\n",
    "            'skype:',\n",
    "            'javascript:',\n",
    "            'facebook',\n",
    "            'twitter',\n",
    "            '.tar.gz',\n",
    "            '.exe',\n",
    "            '.apk',\n",
    "        }\n",
    "\n",
    "    def URLLength(this):\n",
    "        return len(this.URL.Hostname + this.URL.Path)\n",
    "\n",
    "    def URLType(this):\n",
    "        subdomains = tldextract.extract(this.URL.Hostname).subdomain\n",
    "        subdomains = subdomains.split('.')\n",
    "        # first remove exceptions from the list\n",
    "        excludes = {\n",
    "            'www',\n",
    "            'ww1',\n",
    "            'ww2',\n",
    "            '',\n",
    "        }\n",
    "        for exclude in excludes:\n",
    "            if exclude in subdomains:\n",
    "                subdomains.remove(exclude)\n",
    "        if len(subdomains) > 0: # if domain has subdomain, it is of type 2\n",
    "            return 2\n",
    "        else:\n",
    "            return 1 # else we default to type 1. \n",
    "        # note: there is no way to check if a URL is of type 3 programatically; see discusson in thesis document\n",
    "\n",
    "    def GetTopDomain(this):\n",
    "        extract = tldextract.extract(this.URL.Hostname)\n",
    "        return extract.domain + '.' + extract.suffix\n",
    "\n",
    "\n",
    "    # returns all hyperlinks that point to inbound domains (relative paths = inbound)\n",
    "    def GetInboundURLs(this):\n",
    "        domain = this.URL.Hostname\n",
    "        URLs = this.Tree.xpath('(//a)[contains(@href, \"' + domain + '\") or not(contains(@href, \"http\"))]/@href')\n",
    "\n",
    "        urls_found = []\n",
    "        result     = []\n",
    "        for url in URLs:\n",
    "            try:\n",
    "                excluded = False\n",
    "                for exclude in this.Excludes:\n",
    "                    if exclude in url.lower():\n",
    "                        excluded = True\n",
    "                        break\n",
    "                if not excluded:\n",
    "                    if domain in url:\n",
    "                        url = BooterURL(url)\n",
    "                        url = url.Hostname + url.Path\n",
    "                        if url[len(url) - 1] == '/':\n",
    "                            url = url[:-1]\n",
    "                        if url not in urls_found:\n",
    "                            result.append(url)\n",
    "                    else:\n",
    "                        url = (this.RelativeURL + '/' + url).replace('//', '/')\n",
    "                        if url[len(url) - 1] == '/':\n",
    "                            url = url[:-1]\n",
    "                        if url not in urls_found:\n",
    "                            result.append(url)\n",
    "                    urls_found.append(url) # to check for duplicates\n",
    "            except Exception as ex:\n",
    "                pass\n",
    "\n",
    "        return result\n",
    "\n",
    "    # returns all outbound hyperlinks \n",
    "    def GetOutboundURLs(this):\n",
    "        domain = this.URL.Hostname\n",
    "        URLs = this.Tree.xpath('(//a)[contains(@href, \"http\") and not(contains(@href, \"' + domain + '\"))]/@href')\n",
    "\n",
    "\n",
    "        urls_found = []\n",
    "        result = []\n",
    "        for url in URLs:\n",
    "            excluded = False\n",
    "            for exclude in this.Excludes:\n",
    "                if exclude in url.lower():\n",
    "                    excluded = True\n",
    "                    break\n",
    "            if not excluded:\t\n",
    "                if domain in url:\n",
    "                    if url not in urls_found:\n",
    "                        result.append(url)\n",
    "                else:\t\t\n",
    "                    if url not in urls_found:\n",
    "                        result.append(url)\n",
    "                urls_found.append(url) # to check for duplicates\n",
    "        return result\n",
    "\n",
    "\n",
    "    # returns a tokenized list of words/phrases found in the crawled page's content\n",
    "    def GetContent(this):\n",
    "        text_content = []\n",
    "        if len(this.HTML) < 250000: # don't run xPath on too large HTML pages, takes ages (slightly bias-ed but otherwise destroys crawler times)\n",
    "            content      = this.Tree.xpath('(//p)[not(contains(@style, \"hidden\"))]/descendant-or-self::node()/text() | (//div)[not(contains(@style, \"hidden\"))]//descendant-or-self::node()[not(descendant-or-self::p) and not(descendant-or-self::script) and not (descendant-or-self::style)]/text()')\n",
    "            for text in content:\n",
    "                # first remove irelevant characters/symbols\n",
    "                remove = { '\\\\t', '\\\\r', '\\\\n', '&nbsp;' }\n",
    "                for removeable in remove:\n",
    "                    text = text.replace(removeable, '')\n",
    "                # then split text by whitespace and add each entry to text_content\n",
    "                text_content.append(text.split())\n",
    "\n",
    "            # merge all lists in text_content list into one final list of words\n",
    "            text_content = [item for sublist in text_content for item in sublist]\n",
    "        else:\n",
    "            # add bogus content to text_content (constant set as 1000)\n",
    "            for i in range(0, 1000):\n",
    "                text_content.append('too_large_html')\n",
    "\n",
    "        return text_content\n",
    "\n",
    "    # returns a bit-wise result whether this page contains an HTML login form (or register)\n",
    "    def HasLoginForm(this):\n",
    "        forms = this.Tree.xpath('(//form)//input[contains(@type, \"password\")]')\n",
    "        return len(forms) > 0\n",
    "\n",
    "\n",
    "    def __str__(this):\n",
    "        return this.URL.Full_URL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"2\"><h1><a href=\"#TOC\">2. Crawling Google</a></h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[2mCRAWLER: CRAWLING GOOGLE2\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: initiating crawling procedures\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: {'Accept-Language': 'nl-NL,nl;q=0.8,en-US;q=0.6,en;q=0.4', 'Accept-Enco\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: initiating crawling procedures: Google\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[37m\u001b[2mCRAWLER: KEYWORD: Booter\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://booter.xyz/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://webstresser.co/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: http://quezstresser.com/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: http://webstresser.co/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://www.vbooter.org/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: http://www.databooter.com/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://www.safeskyhacks.com/Forums/showthread.php?39-Top-10-DDo\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://booter.org/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: http://freebooter.co/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://vdos-s.com/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://inboot.me/\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!URL: https://ragebooter.net/\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-cc2af9ef0511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mcrawler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawler_Google2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m102\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# up to ~500 results (more is not possible by Google)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;31m# results_google = crawler.Finish('crawl_google2.txt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-cc2af9ef0511>\u001b[0m in \u001b[0;36mCrawl\u001b[1;34m(this, max_results)\u001b[0m\n\u001b[0;32m     50\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;34m'/url?q='\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'&sa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                         \u001b[0mthis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAddToList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBooterURL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Google'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msplit\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-357794d2864d>\u001b[0m in \u001b[0;36mAddToList\u001b[1;34m(this, URL, source)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[1;31m# get status/respnse-code and resolved-url of URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFull_URL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m                 \u001b[0mURL\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mBooterURL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;31m# save in database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-357794d2864d>\u001b[0m in \u001b[0;36mGetStatus\u001b[1;34m(this, url)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;31m# is offline/online or for sale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mGetStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSCrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m403\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m202\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;31m# check if for sale, otherwise site deemed as online\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-357794d2864d>\u001b[0m in \u001b[0;36mJSCrawl\u001b[1;34m(this, url)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mJSCrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mjs_scraper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcfscrape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_scraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjs_scraper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHeader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m         \u001b[0mredirect_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;34m'http-equiv=\"Refresh\"'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/cfscrape/__init__.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Check if Cloudflare anti-bot is on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m503\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Server\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"cloudflare-nginx\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve_cf_challenge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Otherwise, no Cloudflare anti-bot detected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/cfscrape/__init__.pyc\u001b[0m in \u001b[0;36msolve_cf_challenge\u001b[1;34m(self, resp, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msolve_cf_challenge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Cloudflare requires a delay before solving the challenge\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "from colorama import Fore, Back, Style\n",
    "import json\n",
    "\n",
    "# documentation: https://developers.google.com/web-search/docs/#The_Basics\n",
    "\n",
    "class Crawler_Google2(Crawler):\n",
    "    'Crawler of Google via default web requests'\n",
    "    def __init__(this, sleep_level=1):\n",
    "        domain = 'https://www.google.com/search?ie=UTF-8&num=100' \n",
    "        Crawler.__init__(this, domain, sleep_level)\n",
    "        this.PrintNote('CRAWLING GOOGLE2')\n",
    "        this.PrintDivider()\n",
    "        this.Initialize()\n",
    "        # this.Header['referer'] = 'utwente.nl'\n",
    "\n",
    "    # def Login(this):\n",
    "        # no login\n",
    "        # this.PrintError('NO LOGIN REQUIRED')\n",
    "\n",
    "    # overrides Crawler's crawl function\n",
    "    def Crawl(this, max_results=100):\n",
    "        keywords = ['Booter', 'DDOSer', 'Stresser']\n",
    "\n",
    "        nr_pages = int(max_results / 100)\n",
    "        this.PrintUpdate('initiating crawling procedures: Google')\n",
    "\n",
    "        for keyword in keywords:\n",
    "            this.PrintDivider()\n",
    "            this.PrintNote('KEYWORD: ' + keyword)\n",
    "            this.PrintDivider()\n",
    "            for i in range(0, nr_pages): \n",
    "                counter = 0\n",
    "                # dynamically generate search query\n",
    "                query =  \"&q=\" + keyword+ '&start=' + str(i * 100) + '&filter=0' \n",
    "                url = this.Target + query\n",
    "\n",
    "                # read html and parse JSON\n",
    "                response = this.JSCrawl(url)                 \n",
    "                tree \t = html.fromstring(response.text) \n",
    "\n",
    "                # urls = tree.xpath('(//li)[@class=\"g\"]//a[not(contains(@href, \"translate\"))]/text()')\n",
    "                urls = tree.xpath('(//div)[@class=\"g\"]//h3[@class=\"r\"]/a/@href')\n",
    "\n",
    "#WHY DO YOU SPLIT IN THEN?!\n",
    "                split   = 10\n",
    "                for url in urls:\n",
    "                    this.PrintDebug('!!URL: ' + BooterURL(url).Full_URL)  \n",
    "                    try:\n",
    "                        # parse url\n",
    "                        if '/url?q=' in url:\n",
    "                            url = url[7:].split('&sa')[0]\n",
    "                        this.AddToList(BooterURL(url), 'Google')\n",
    "\n",
    "                        if counter % split == 0:\n",
    "                            this.PrintDivider()\n",
    "                        counter = counter + 1\n",
    "                    except Exception as ex:\n",
    "                        this.PrintError('EXCEPTION: ' + str(ex))\n",
    "                this.Sleep()\n",
    "\n",
    "\n",
    "        this.PrintUpdate('DONE; found ' + str(len(this.URLs)) + ' potential Booters')\n",
    "        this.PrintDivider()\n",
    "        \n",
    "\n",
    "crawler = Crawler_Google2(1)\n",
    "crawler.Crawl(102) # up to ~500 results (more is not possible by Google)\n",
    "# results_google = crawler.Finish('crawl_google2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"3\"><h1><a href=\"#TOC\">3. Crawling Youtube</a></h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[2mCRAWLER: CRAWLING YOUTUBE\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: initiating crawling procedures\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: {'Accept-Language': 'nl-NL,nl;q=0.8,en-US;q=0.6,en;q=0.4', 'Accept-Enco\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: initiating crawling procedures: Youtube\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[37m\u001b[2mCRAWLER: KEYWORD: network security\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=0\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://ocw.mit.edu/6-858F14\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://cs.co/6054BPj4O\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://howtonetwork.com/courses/comptia/comptia-network/\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://networksecuritykey.org\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 4 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=1\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://ocw.mit.edu/6-858F14\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://cs.co/6054BPj4O\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://howtonetwork.com/courses/comptia/comptia-network/\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://networksecuritykey.org\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 4 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=2\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://thenewboston.com/videos.php\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.patreon.com/thenewboston\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://technology-howtos.blogspot.in/2011/08/how-can-i-find-networ\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://thenewboston.com/videos.php\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.patreon.com/thenewboston\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 6 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=3\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://css.csail.mit.edu/6.858/2014/\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://bitly.com/SubscribeNews7Tamil\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://fb.com/News7Tamil\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://ifactner.com\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.theextremewebdesigns.com\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://thenewboston.com/videos.php\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.patreon.com/thenewboston\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 7 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=4\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://thenewboston.com/videos.php\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.patreon.com/thenewboston\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://mec.ph/security\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.sophos.com/en-us/lp/network-threats\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://networksecuritykey.org\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 5 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=5\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.siemens.de/industrial-security\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://computersecurity.stanford.edu/courses/network-security\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://thenewboston.com/videos.php\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.patreon.com/thenewboston\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 5 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=6\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.productivecorp.com\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://mayurgaikwad.hwbizhid.click2sell.eu\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.facebook.com/free4arab\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.free4arab.com\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 4 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=7\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://cs.co/9009BHcuc\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://ns3simulator.com/b-tech-final-ye\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://ifactner.com\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.ecpi.edu/\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://cdw.com/security\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.nsauditor.com/network_security/network_security_auditor\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.facebook.com/free4arab\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 8 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=8\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.trainsignal.com/Certified-Ethical-Hacker.aspx?utm_sourc\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://business.twc.com\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://ifactner.com\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://williscollege.com\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 5 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &search_query=\"network security\"&page=9\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://thenewboston.com/videos.php\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: https://www.patreon.com/thenewboston\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://www.sophos.com/en-us/lp/network-threats\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: !!!: http://asecuritysite.com/ns\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: obtained 4 potential URLs; extracting...\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[31m\u001b[22mCRAWLER: EXCEPTION: table urls has 18 columns but 9 values were supplied\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: DONE; found 0 potential Booters\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: writing output to file 'crawl_youtube.txt'\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: FINISHED; closing operations\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from lxml import html\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "class Crawler_Youtube(Crawler):\n",
    "    'Crawler of Youtube via default web requests'\n",
    "    def __init__(this, sleep_level=1):\n",
    "        domain = 'https://www.youtube.com/results?' \n",
    "        Crawler.__init__(this, domain, sleep_level)\n",
    "\n",
    "        this.PrintNote('CRAWLING YOUTUBE')\n",
    "        this.PrintDivider()\n",
    "        this.Initialize()\n",
    "\n",
    "    # def Login(this):\n",
    "        # no login\n",
    "        # this.PrintError('NO LOGIN REQUIRED')\n",
    "\n",
    "    # overrides Crawler's crawl function\n",
    "    def Crawl(this, max_results=100):\n",
    "#jjsantanna         keywords = ['online booter', 'stresser', 'ddoser']\n",
    "        keywords = ['network security']\n",
    "\n",
    "        nr_pages = int(max_results / 10)\n",
    "        \n",
    "\n",
    "        this.PrintUpdate('initiating crawling procedures: Youtube')\n",
    "\n",
    "        for keyword in keywords:\n",
    "            this.PrintDivider()\n",
    "            this.PrintNote('KEYWORD: ' + keyword)\n",
    "            for i in range(0, nr_pages): \n",
    "                counter = 0\n",
    "                try:\n",
    "                    # dynamically generate search query\n",
    "                    query =  '&search_query=\"' + keyword\t+ '\"&page=' + str(i)\n",
    "                    url = this.Target + query\n",
    "                    this.PrintDivider()\n",
    "                    this.PrintDebug('crawling: ' + query) \n",
    "                    # read html and parse JSON\n",
    "                    response = this.JSCrawl(url)\n",
    "                    tree \t = html.fromstring(response.text) \n",
    "                    split   = 10\n",
    "\n",
    "                    urls_found = []\n",
    "\n",
    "                    descriptions = tree.xpath('(//div)[contains(@class, \"yt-lockup-description\")]/descendant-or-self::*/text()')\t\t\t\t\t\n",
    "                    \n",
    "                    for description in descriptions:\n",
    "                        # check whether description certainly doesn't hold an online booter\n",
    "                        if this.StopSearching(description):\n",
    "                            continue\n",
    "\n",
    "                        # find all urls in description\n",
    "                        urls = re.findall('http[s]?:\\/\\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', description)\n",
    "                        for url in urls:\n",
    "                            urls_found.append(url)\n",
    "\n",
    "\n",
    "                    # also check for explicit urls in descriptions\n",
    "                    urls = tree.xpath('(//div)[contains(@class, \"yt-lockup-description\")]//a/@href')\t\t\t\t\t\n",
    "                    for url in urls:\n",
    "                        this.PrintDebug('!!!: '+str(url))\n",
    "                        if url not in urls_found:\n",
    "                            urls_found.append(url)\n",
    "                            \n",
    "\n",
    "                    this.PrintDivider()\n",
    "                    this.PrintUpdate('obtained ' + str(len(urls_found)) + ' potential URLs; extracting...')\n",
    "                    this.PrintDivider()\n",
    "\n",
    "                    # resolve each url and add returned url to final urls\n",
    "                    for url in urls_found:\n",
    "                        this.AddToList(BooterURL(url), 'Youtube')\n",
    "                        counter = counter + 1\n",
    "                        if counter % split == 0:\n",
    "                            this.PrintDivider()\t\n",
    "\n",
    "                    this.Sleep()\n",
    "                except Exception as ex:\n",
    "                    this.PrintError('EXCEPTION: ' + str(ex))\n",
    "                    this.Sleep()\n",
    "\n",
    "        this.PrintUpdate('DONE; found ' + str(len(this.URLs)) + ' potential Booters')\n",
    "        this.PrintDivider()\n",
    "\n",
    "\n",
    "    # decides to stop crawling a specific description as soon as\n",
    "    # certain stop keywords are found like 'tutorial'\n",
    "    def StopSearching(this, description):\n",
    "        stop_words = {\n",
    "            'tutorial'\n",
    "            'download:'\n",
    "            'gui',\n",
    "            'dekstop'\n",
    "        }\n",
    "        text = description.lower()\n",
    "        for word in stop_words:\n",
    "            if word in text:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "#################    \n",
    "# USAGE EXAMPLE #\n",
    "#################\n",
    "crawler = Crawler_Youtube(1)\n",
    "crawler.Crawl(100) # up to ~500 results (similar limit to Google)\n",
    "crawler.Finish('crawl_youtube.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"4\"><h1><a href=\"#TOC\">4. Crawling Hackerforums.net</a></h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "class Crawler_Hackforums(Crawler):\n",
    "    'Crawler for web services of www.hackforums.net'\n",
    "    def __init__(self, sleep_level=1):\n",
    "        domain = 'http://www.hackforums.net/' \n",
    "        Crawler.__init__(self, domain, sleep_level)\n",
    "\n",
    "        self.PrintNote('CRAWLING HACKFORUMS')\n",
    "        self.PrintDivider()\n",
    "        self.Initialize()\n",
    "\n",
    "    # Login to hackforums.net\n",
    "    # NOTE: somtimes their login procedures block automated attempts; in that case a semi-brute\n",
    "    # force attempt is executed in the Crawler superclass.\n",
    "    def Login(self):\n",
    "        username = 'your_username_here' \n",
    "        password = 'your_password_here'\n",
    "        url = self.Target + 'member.php'\n",
    "        post_data = {\n",
    "            'username': username,\n",
    "            'password': password,\n",
    "            'action': 'do_login',\n",
    "            'url': 'http://www.hackforums.net/index.php',\n",
    "            'my_post_key': '60aae6001602ef2e0bd45033d53f53dd'\n",
    "        }\n",
    "        return super(Crawler_Hackforums, self).Login(\n",
    "            url, \n",
    "            self.Target + 'index.php', \n",
    "            post_data\n",
    "        )\n",
    "\n",
    "    # overrides Crawler's crawl function\n",
    "    def Crawl(self, max_date):\n",
    "        # crawl of hackforums.net is executed in three steps:\n",
    "        # 1. we retrieve all interesting forum posts\n",
    "        # 2. we extract potential Booter URLs from these posts\n",
    "        # 3. collect all related evidence and calculate scores \n",
    "        #    for later evaluation\n",
    "        target_url = self.Target + 'forumdisplay.php?fid=232&page='\n",
    "        ### step 1. retrieve all relevant forum posts\n",
    "        forum_items  \t = []\n",
    "        current_page \t = 1\n",
    "        max_pages \t \t = 1\n",
    "        max_date_reached = False\n",
    "        self.PrintUpdate('initiating crawling procedures: HackForums')\n",
    "        self.PrintDivider()\n",
    "\n",
    "        # crawl first forum page and parse into XML tree\n",
    "        response = self.Session.post(target_url + str(current_page), headers=self.Header)\n",
    "        tree \t = html.fromstring(response.text) \n",
    "\n",
    "        # analyze structure and get relevant properties (via XPATH)\n",
    "        self.PrintUpdate('analyzing structure and retrieving Booter candidates')\n",
    "        self.PrintDivider()\n",
    "        max_pages = int(tree.xpath('//a[@class=\"pagination_last\"]/text()')[0])\n",
    "        # max_pages = 1 # for debug\n",
    "        # now start crawling\n",
    "        while current_page <= max_pages and not max_date_reached:\n",
    "            self.PrintUpdate('crawling page ' + str(current_page) + '/' + str(max_pages))\n",
    "            self.PrintDivider()\n",
    "            # get forum items\n",
    "            forum_titles = tree.xpath('//td[contains(@class,\"forumdisplay_\")]/div/span[1]//a[contains(@class, \" subject_\")]/text()')\n",
    "            forum_urls   = tree.xpath('//td[contains(@class,\"forumdisplay_\")]/div/span[1]//a[contains(@class, \" subject_\")]/@href')\n",
    "            forum_dates  = tree.xpath('//td[contains(@class,\"forumdisplay_\")]/span/text()[1]')\n",
    "            # get data of each forum item\n",
    "            for i in range(len(forum_titles)):\n",
    "                item = ForumItem(forum_titles[i], self.Target + forum_urls[i], forum_dates[i])\n",
    "                if item.IsPotentialBooter():\n",
    "                    forum_items.append(item)\n",
    "                    print(item)\n",
    "                # check if max date is reached\n",
    "                if item.Date < max_date:\n",
    "                    max_date_reached = True\n",
    "                    self.PrintDivider()\n",
    "                    self.PrintUpdate('date limit reached; aborting...')\n",
    "                    self.PrintDivider()\n",
    "                    break\n",
    "            # print a divider after each forum page\n",
    "            self.PrintDivider()\n",
    "            # get url of next page and re-iterate\n",
    "            current_page = current_page + 1\n",
    "            next_url     = target_url + str(current_page)\n",
    "            response     = self.Session.post(next_url, headers=self.Header)\n",
    "            tree         = html.fromstring(response.text)            \n",
    "\n",
    "            if current_page <= max_pages:\n",
    "                self.Sleep()\n",
    "        # forum crawling is complete, print (sub)results\n",
    "        self.PrintUpdate('items found: ' + str(len(forum_items)))\n",
    "        self.PrintDivider()\n",
    "\n",
    "        ### step 2. extract potential Booters from target forum posts\n",
    "        self.PrintUpdate('attempting to obtain Booter URLs')\n",
    "        self.PrintDivider()\n",
    "        # start crawilng for each forum item\n",
    "        counter = 0\n",
    "        for item in forum_items:\n",
    "            # parse html\n",
    "            response = self.Session.post(item.URL, headers=self.Header)\n",
    "            tree \t = html.fromstring(response.text)\n",
    "            url \t = ''\n",
    "            # check for URLs inside image tags\n",
    "            tree_image = tree.xpath('(//tbody)[1]//div[contains(@class,\"post_body\")]//a[.//img and not(contains(@href, \"hackforums.net\")) and not(contains(@href, \".jpg\") or contains(@href, \".png\") or contains(@href, \".jpeg\") or contains(@href, \"gif\"))]/@href')\n",
    "            if tree_image:\n",
    "                url = tree_image[0]\n",
    "            else:\n",
    "                # otherwise check for URL in the post's content\n",
    "                tree_links = tree.xpath('(//tbody)[1]//div[contains(@class,\"post_body\")]//a[not(@onclick) and not(contains(@href, \"hackforums.net\")) and not(contains(@href, \".jpg\") or contains(@href, \".png\") or contains(@href, \".jpeg\") or contains(@href, \".gif\"))]/@href')\n",
    "                if tree_links:\n",
    "                    url = tree_links[0]\n",
    "\n",
    "            # add found url to list\n",
    "            if url != '':\n",
    "                self.AddToList(BooterURL(url), item.URL)\n",
    "\n",
    "            # print a divider line every 10 results (to keep things organized)\n",
    "            counter = counter + 1\n",
    "            if counter % 10 == 0:        \n",
    "                self.PrintDivider()\n",
    "\n",
    "        # finished, print results\n",
    "        self.PrintDivider()\n",
    "        self.PrintUpdate('DONE; Resolved: ' + str(len(self.URLs)) + ' Booter URLs')\n",
    "        self.PrintDivider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cralwer.py [MAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[2mCRAWLER: CRAWLING GOOGLE2\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: initiating crawling procedures\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: {'Accept-Language': 'nl-NL,nl;q=0.8,en-US;q=0.6,en;q=0.4', 'Accept-Enco\u001b[0m\n",
      "\u001b[32m\u001b[22mCRAWLER: initiating crawling procedures: Google\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[37m\u001b[2mCRAWLER: KEYWORD: Booter\u001b[0m\n",
      "\u001b[33m\u001b[22m................................................................................\u001b[0m\n",
      "\u001b[36m\u001b[22mCRAWLER: crawling: &q=Booter&start=0&filter=0\u001b[0m\n",
      "GLOBAL EXCEPTION: Could not find an available JavaScript runtime.\n"
     ]
    }
   ],
   "source": [
    "from urlparse import urlparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "results_google \t   = []\n",
    "results_hackforums = []\n",
    "results_youtube\t   = []\n",
    "\n",
    "try:\n",
    "\t###############################################################################\n",
    "\t## GOOGLE V2                                                                 ##\n",
    "\t## ############################################################################\n",
    "\tcrawler = Crawler_Google2(1)\n",
    "\tcrawler.Crawl(100) # up to ~500 results (more is not possible by Google)\n",
    "\tresults_google = crawler.Finish('crawl_google2.txt')\n",
    "\n",
    "\tprint()\n",
    "\tprint()\n",
    "\tprint()\n",
    "\n",
    "\t###############################################################################\n",
    "\t## YOUTUBE                                                                   ##\n",
    "\t## ############################################################################\n",
    "\tcrawler = Crawler_Youtube(1)\n",
    "\tcrawler.Crawl(100) # up to ~500 results (similar limit to Google)\n",
    "\tresults_youtube = crawler.Finish('crawl_youtube.txt')\n",
    "\n",
    "\tprint()\n",
    "\tprint()\n",
    "\tprint()\n",
    "\n",
    "\t###############################################################################\n",
    "\t## HACKFORUMS                                                                ##\n",
    "\t###############################################################################\n",
    "\tcrawler = Crawler_Hackforums(1) # sleep level of 1\n",
    "\tif crawler.Login():\t# from time to time it might give a 5 sec check browser period; if so, simply manually solve this by visiting hackforums.net\n",
    "\t\tprint('login succesfull')\n",
    "\t\tcrawler.Crawl(datetime.datetime(2015, 3, 1)) # crawl up to may 15th\n",
    "\t\tresults_hackforums = crawler.Finish('crawl_hackforums.txt')\n",
    "\n",
    "\tprint()\n",
    "\tprint()\n",
    "\tprint()\n",
    "except Exception as ex:\n",
    "\tprint('GLOBAL EXCEPTION: ' + str(ex))\n",
    "\n",
    "###############################################################################\n",
    "## MERGE CRAWL RESULTS                                                       ##\n",
    "###############################################################################\n",
    "# final_results = []\n",
    "\n",
    "# def IsDuplicate(booterURL):\n",
    "# \tfor i in range(0, len(final_results)):\n",
    "# \t\tif booterURL.UniqueName() == final_results[i].UniqueName():\n",
    "# \t\t\treturn True\n",
    "# \treturn False\t\t\n",
    "\n",
    "# def AddResults(sub_results):\n",
    "# \tfor booterURL in sub_results:\n",
    "# \t\t# we make the assumption the uniqueness of a Booter url is based on its\n",
    "# \t\t# hostname only; thus we only have TYPE 1 URLs (prove this!)\n",
    "# \t\tif not IsDuplicate(booterURL):\n",
    "# \t\t\tfinal_results.append(booterURL)\n",
    "# \t\telse:\n",
    "# \t\t\tprint('duplicate:' + booterURL.Full_URL)\n",
    "\n",
    "# AddResults(results_google)\n",
    "# AddResults(results_youtube)\n",
    "# AddResults(results_hackforums)\n",
    "\n",
    "# crawler.PrintDivider();\n",
    "# crawler.PrintUpdate('completed crawling procedures; saving output to \\'crawler_output.txt\\'')\n",
    "# with open('crawler_output.txt', 'w') as f:\n",
    "\t# for booterURL in final_results:\n",
    "\t\t# f.write(booterURL.UniqueName() + '\\n')\n",
    "\t\t# json.dump(vars(booterURL), f)\n",
    "\t\t# f.write('\\n')\n",
    "\n",
    "###############################################################################\n",
    "## SCRAPE AND GENERATE SCORES                                                ##\n",
    "###############################################################################\n",
    "crawler.PrintDivider();\n",
    "crawler.PrintUpdate('INITIATING SCRAPING PROCEDURES;')\n",
    "crawler.PrintDivider();\n",
    "\n",
    "# query all to-scrape URLs\n",
    "# from_date    = datetime.datetime(2015, 8, 1).strftime('%Y-%m-%d %H:%M:%S') # test_scores\n",
    "# from_date    = datetime.datetime(2015, 8, 19).strftime('%Y-%m-%d %H:%M:%S') #test_scores2\n",
    "from_date    = datetime.datetime(2015, 8, 20, 13, 30).strftime('%Y-%m-%d %H:%M:%S') #test_scores3\n",
    "delay_period = 7\n",
    "\n",
    "for url in Select('SELECT fullURL FROM urls WHERE status != \\'off\\' AND timeUpdate >= \\'' + str(from_date) + '\\''):\n",
    "\tdelay = delay_period + random.randint(0,14) # add a slight randomness to delay_period as to divide workload\n",
    "\tdelay = 1\n",
    "\tcrawler.Scrape(BooterURL(url[0]), delay)\n",
    "\n",
    "crawler.PrintDivider();\n",
    "crawler.PrintUpdate('DONE;')\n",
    "crawler.PrintDivider();\n",
    "\n",
    "\n",
    "# TESTING SCRAPE ALGORITHM\n",
    "# crawler.Scrape(BooterURL('http://joeydevries.com'))\n",
    "# crawler.Scrape(BooterURL('k-stress.pw'))\n",
    "# crawler.Scrape(BooterURL('http://impulse101.org/'))\n",
    "# crawler.Scrape(BooterURL('http://www.davidairey.com/'))\n",
    "# crawler.Scrape(BooterURL('booter.org'))\n",
    "# crawler.Scrape(BooterURL('joeyfladderak.com/'))\n",
    "# crawler.Scrape(BooterURL('layer7.pw'))\n",
    "# crawler.Scrape(BooterURL('tweakers.net'))\n",
    "# crawler.Scrape(BooterURL('ragebooter.com'))\n",
    "# crawler.Scrape(BooterURL('nl.urbandictionary.com'))\n",
    "# crawler.Scrape(BooterURL('http://www.afkortingen.nu/ddos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
