{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Booter (Black)List [all in one]</h1> \n",
    "\n",
    "This is a <a href=\"http://jupyter.org/\">Jupyther Notebook</a> that merges classes, functions and python scripts (all in one single place). Our goal is to collect an extensive list of Booter Websites. Each \"cell\" in this notebook is an independent part of the code towards the colection of a Booter list. For this reason we \"import\" needed libraries per \"cell\" to facilitate your understanding on the requirements. \n",
    "\n",
    "ANYONE can reproduce our work and generate their own Booter (black)List. We particularly use the methodology described in this notebook to generate the most extensive list of Booters (available at <a href=\"http://booterblacklist.com\">booterblacklist.com</a>). The main difference between our list and the list generated using this methodology is that we (also) make available Booter websites that WERE online in the past but disapeared from the Internet (and therefore the crawler will not find). We also have access to ALL the registered domains in the \".com\", \".net\", \".org\", and \".nl\" Top Level Domains (TLD), which we use to classify whether a domain is a Booter (or not). Do not hesitate to <a href=\"mailto:j.j.santanna@utwente.nl\">contact us</a> if you have any question. We would appreciate your comments and suggestions to improve our methodology. Join us!\n",
    "\n",
    "An academic paper that describes our entire methodology is under reviewing process. Our methodology rely on two parts: (1) to collect an extensive list of URLs potentially related to Booter websites and afterwards (2) to classify the collected list in the previous part whether it is a Booter website or not. We split each part in several other subparts as following:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div id=\"TOC\">\n",
    "<ul>\n",
    "<li><a href=\"#1\"><b>1. Defining functions to colect and store information of URLs potential related to Booters' website</b></a></li>\n",
    "<ul>\n",
    "    <li><a href=\"#1.1\">1.1. Database functions:</a> defines several functions to interact with the database that stores the information of each URL potential related to a Booter; </li>\n",
    "    <li><a href=\"#1.2\">1.2. Partitioning URL(s):</a> this class extracts several attributes that are parts of a URL (input);</li>\n",
    "    <li><a href=\"#1.3\">1.3. General Scraper:</a> get general information from a Web page;</li>\n",
    "    <li><a href=\"#1.4\">1.4. Crawler & in-depth Scraper:</a> pretends to be a browser to retrieve information of webpages; collects ONLY information of potential Booter websites;</li>\n",
    "    <li><a href=\"#1.5\">1.5. Refining our Crawler/Scraper</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#1.5.1\">1.5.1. Applied to Google </a></li>\n",
    "    <li><a href=\"#1.5.2\">1.5.2. Applied to Youtube:</a> search for URLs in the description of videos found using Booter-related keywords;</li>\n",
    "    <li><a href=\"#1.5.3\">1.5.3. Applied to Hackerforum.net </a></li>\n",
    "    </ul>\n",
    "</ul>\n",
    "<li><a href=\"#X\"><b>!!! Instantiating the Crawlers and collecting the list of URL potentially related to Booters websites.</b></a></li>\n",
    "\n",
    "</ul>\n",
    "</div>\n",
    "<br>\n",
    "[ATTENTION: I'm still moving the classifier to this Notebook. If you can't wait you can see directly in the folder \"Classifier\".]\n",
    "<br>\n",
    "<br>\n",
    "This research project started at the University of Twente, The Netherlands, by <a href=\"http://jairsantanna.com\">Jair Santanna</a> with collaboration of Justyna Chromik and <a href=\"https://github.com/JoeyDeVries\"> Joey de Vries</a>. \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1\"><h1><a href=\"#TOC\">1. Defining functions to colect and store information of URLs potential related to Booters' website</a></h1></li>\n",
    "\n",
    "<div id=\"1.1\"><h2><a href=\"#TOC\">1.1. Database functions</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "# Checking if the Database exist! If not creates a new 'Booters.db' database.\n",
    "if (os.path.isfile('BOOTERS.db')) == False:\n",
    "    connection = sqlite3.connect('BOOTERS.db')\n",
    "    c = connection.cursor()\n",
    "    c.execute(\"CREATE TABLE urls ( domainName,\\\n",
    "                                fullURL,\\\n",
    "                                timeUpdate,\\\n",
    "                                'booter?',\\\n",
    "                                status,\\\n",
    "                                srcInformation,\\\n",
    "                                timeAdd,\\\n",
    "                                classification,\\\n",
    "                                notes)\") \n",
    "    c.execute(\"CREATE TABLE scores ( URL,\\\n",
    "                                timeUpdate,\\\n",
    "                                numberPages,\\\n",
    "                                urlType,\\\n",
    "                                averageDepthLevel,\\\n",
    "                                averageUrlLength,\\\n",
    "                                domainAge,\\\n",
    "                                domainReservationDuration,\\\n",
    "                                whoisPrivate,\\\n",
    "                                dps,\\\n",
    "                                pageRank,\\\n",
    "                                averageContentSize,\\\n",
    "                                outboundHyperlinks,\\\n",
    "                                categorySpecificDictionary,\\\n",
    "                                resolverIndication,\\\n",
    "                                termsServicesPage,\\\n",
    "                                loginFormDepthLevel)\")\n",
    "    c.execute(\"CREATE TABLE characteristics (URL,\\\n",
    "                                        timeUpdate,\\\n",
    "                                        numberPagesRaw,\\\n",
    "                                        urlTypeRaw,\\\n",
    "                                        averageDepthLevelRaw,\\\n",
    "                                        averageUrlLengthRaw,\\\n",
    "                                        domainAgeRaw,\\\n",
    "                                        domainReservationDurationRaw,\\\n",
    "                                        whoisPrivate,\\\n",
    "                                        dps,\\\n",
    "                                        pageRankRaw,\\\n",
    "                                        averageContentSizeRaw,\\\n",
    "                                        outboundHyperlinksRaw,\\\n",
    "                                        categorySpecificDictionaryRaw,\\\n",
    "                                        resolverIndication,\\\n",
    "                                        termsServicesPage,\\\n",
    "                                        loginFormDepthLevelRaw)\")\n",
    "    connection.commit()\n",
    "    connection.close()\n",
    "\n",
    "# Open connection and retrieve (single) cursor\n",
    "connection = sqlite3.connect('BOOTERS.db') \n",
    "\n",
    "# saves a Booter URL in the database. If the Booter URL was not yet found a row is inserted,\n",
    "# otherwise updated.\n",
    "def SaveURL(booterURL, source = '', status='?', notes=''):\n",
    "    url = booterURL.URL\n",
    "    url_unique = booterURL.UniqueName()\n",
    "    # Check if booter's url already exists\n",
    "    if RowExists('urls', url_unique):\n",
    "        # If entry exists, only do necessary updates \n",
    "        Update('urls', url_unique, 'timeUpdate', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')) # so we can see which URLS are from august 1 onwards (test set)\n",
    "        # Also append source information if not yet stored\n",
    "        sources = GetSingleValue('urls', url_unique, 'srcInformation')\n",
    "        if source not in sources:\n",
    "            Update('urls', url_unique, 'srcInformation', source if sources == '' else sources + ';' + source)\n",
    "    else:\n",
    "        # else, insert into database\n",
    "        Insert('urls', [url_unique, url, 'CURRENT_DATE', '?', status, source, 'CURRENT_DATE', 'A', notes])\n",
    "\n",
    "# Saves a score vector of a single Booter in the database if the score vector was not yet \n",
    "# found a row is inserted, otherwise updated.\n",
    "def SaveScore(table, booterURL, \n",
    "    last_update, nr_pages, url_type, average_depth_level, average_url_length, domain_age, \n",
    "    domain_reservation_duration, whois_private, dps, page_rank, average_content_size, \n",
    "    outbound_hyperlinks, category_specific_dictionary, resolver_indication, terms_of_services_page,\n",
    "    login_form_depth_level):\n",
    "    url = booterURL.URL\n",
    "    url_unique = booterURL.UniqueName()\n",
    "    # check if booter's url already exists\n",
    "    if RowExists(table, url_unique):\n",
    "        # if entry exists, only do a necessary updates\n",
    "        Update(table, url_unique, 'lastUpdate', last_update) \n",
    "        Update(table, url_unique, 'nr_pages', nr_pages) \n",
    "        Update(table, url_unique, 'url_type', url_type) \n",
    "        Update(table, url_unique, 'average_depth_level', average_depth_level) \n",
    "        Update(table, url_unique, 'average_url_length', average_url_length) \n",
    "        Update(table, url_unique, 'domain_age', domain_age) \n",
    "        Update(table, url_unique, 'domain_reservation_duration', domain_reservation_duration) \n",
    "        Update(table, url_unique, 'whois_private', whois_private) \n",
    "        Update(table, url_unique, 'dps', dps) \n",
    "        Update(table, url_unique, 'page_rank', page_rank) \n",
    "        Update(table, url_unique, 'average_content_size', average_content_size) \n",
    "        Update(table, url_unique, 'outbound_hyperlinks', outbound_hyperlinks) \n",
    "        Update(table, url_unique, 'category_specific_dictionary', category_specific_dictionary) \n",
    "        Update(table, url_unique, 'resolver_indication', resolver_indication) \n",
    "        Update(table, url_unique, 'terms_of_services_page', terms_of_services_page) \n",
    "        Update(table, url_unique, 'login_form_depth_level', login_form_depth_level) \n",
    "    else:\n",
    "        # else, insert into database\n",
    "        Insert(table, [url_unique, last_update, nr_pages, url_type, average_depth_level,\n",
    "            average_url_length, domain_age, domain_reservation_duration, whois_private, dps,\n",
    "            page_rank, average_content_size, outbound_hyperlinks, category_specific_dictionary,\n",
    "            resolver_indication, terms_of_services_page, login_form_depth_level])\t\n",
    "\n",
    "# Checks whether a row/entry already exists by comparing a specific column with a check_value for uniqueness\n",
    "def RowExists(table, url):\n",
    "    query = 'SELECT domainName FROM ' + table + ' WHERE domainName = \\'' + url + '\\''\n",
    "    cursor = connection.execute(query)\n",
    "    if cursor.fetchall():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Returns a single value of a single column from the database\n",
    "def GetSingleValue(table, key_value, column, key_column = 'domainName'):\n",
    "    query = 'SELECT ' + column + ' FROM ' + table + ' WHERE ' + key_column + ' = \\'' + key_value + '\\''\n",
    "    cursor = connection.execute(query)\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "# For easy insert statements\n",
    "def Insert(table, values):\n",
    "    query = 'INSERT INTO ' + table + ' VALUES ('\n",
    "    for value in values:\n",
    "        if value == 'CURRENT_DATE':\n",
    "            query += value + ', '\n",
    "        else:\n",
    "            query += '\\'' + str(value) + '\\'' + ', '\n",
    "    query = query[:-2] + ')'\n",
    "    connection.execute(query)\n",
    "    connection.commit()\n",
    "\n",
    "# For easy update statements\n",
    "def Update(table, key, column, value):\n",
    "    query = 'UPDATE ' + table + ' SET ' + column + ' = \\'' + str(value) + '\\' WHERE domainName = \\'' + key + '\\''\n",
    "    connection.execute(query)\n",
    "    connection.commit()\n",
    "\n",
    "def Select(query):\n",
    "    result = []\n",
    "    for row in connection.execute(query):\n",
    "        result.append(row)\n",
    "    return result\n",
    "\n",
    "def CloseConnection():\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.2\"><h2><a href=\"#TOC\">1.2. Partitioning URL</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "\n",
    "# container format for potential Booter URL/domainname (PBD)\n",
    "# Holds hostname/domain, complete URL and easy access to other relevant data\n",
    "class BooterURL:\n",
    "    # constructor\n",
    "    def __init__(this, url):\n",
    "        # if url does not contain protocol; add it.\n",
    "        if 'http' not in url:\n",
    "            url = 'http://' + url\n",
    "        # parse URL and store relevant data\n",
    "        parsed  = urlparse(url)\n",
    "        this.Hostname = parsed.hostname\n",
    "        this.Scheme   = parsed.scheme\n",
    "        this.URL = parsed.scheme + '://' + parsed.netloc + parsed.path\n",
    "        this.Path     = parsed.path\n",
    "        this.Query    = '?' + parsed.query if parsed.query != '' else ''\n",
    "        this.Full_URL = url\n",
    "        this.Status\t  = '?'\n",
    "\n",
    "    # returns a URL representation that uniquely identifies the current URL \n",
    "    # this is the exact format described as a Potential Booter domain name (PBD)\n",
    "    # Type 2 URLs are omitted\n",
    "    def UniqueName(this):\n",
    "        # here we assume the hostname to be a unique identification \n",
    "        protocol = this.Scheme + '://' if this.Scheme else ''\n",
    "        if this.Hostname:\n",
    "            return this.Hostname.replace('www.','')\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def __str__(this):\n",
    "        return this.UniqueName()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.3\"><h2><a href=\"#TOC\">1.3. General Scraper</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import cfscrape\n",
    "import tldextract # https://github.com/john-kurkowski/tldextract\n",
    "from lxml import html\n",
    "\n",
    "# hosts functionality and data relevant to crawling a single web page of a domain. \n",
    "# This includes per-page properties like html content, headers and functionality to retrieve all \n",
    "# inbound and/or outbound URLs and per-category dictionary queries.\n",
    "class CrawlPage:\n",
    "    # constructor\n",
    "    def __init__(this, response):\n",
    "        this.URL     = BooterURL(response.url)\n",
    "        this.HTML \t = response.text\n",
    "        this.Tree\t = html.fromstring(this.HTML)\n",
    "\n",
    "        # create relative path for further URL queries \n",
    "        path = this.URL.Path\n",
    "        if len(path) > 0 and path[0] =='/':\n",
    "            this.RelativeURL = (this.URL.Hostname).replace('//', '/')\t\t\t\n",
    "        else:\n",
    "            for i in reversed(path):\n",
    "                if i == '/':\n",
    "                    break\n",
    "                else:\n",
    "                    path = path[:-1]\n",
    "            this.RelativeURL = (this.URL.Hostname + path).replace('//', '/')\n",
    "        # print('relative:' + this.RelativeURL)\n",
    "\n",
    "        # add a 'contains' list of URLs NOT to scrape\n",
    "        this.Excludes = {\n",
    "            '#',\n",
    "            'mailto',\n",
    "            '.pdf',\n",
    "            '.doc',\n",
    "            '.rar',\n",
    "            '.zip',\t\t\n",
    "            '.png',\n",
    "            '.jpeg',\n",
    "            '.jpg',\n",
    "            '.gif',\n",
    "            '.bmp',\n",
    "            '.atom',\n",
    "            '.rss',\t\n",
    "            'skype:',\n",
    "            'javascript:',\n",
    "            'facebook',\n",
    "            'twitter',\n",
    "            '.tar.gz',\n",
    "            '.exe',\n",
    "            '.apk',\n",
    "        }\n",
    "\n",
    "    def URLLength(this):\n",
    "        return len(this.URL.Hostname + this.URL.Path)\n",
    "\n",
    "    def URLType(this):\n",
    "        subdomains = tldextract.extract(this.URL.Hostname).subdomain\n",
    "        subdomains = subdomains.split('.')\n",
    "        # first remove exceptions from the list\n",
    "        excludes = {\n",
    "            'www',\n",
    "            'ww1',\n",
    "            'ww2',\n",
    "            '',\n",
    "        }\n",
    "        for exclude in excludes:\n",
    "            if exclude in subdomains:\n",
    "                subdomains.remove(exclude)\n",
    "        if len(subdomains) > 0: # if domain has subdomain, it is of type 2\n",
    "            return 2\n",
    "        else:\n",
    "            return 1 # else we default to type 1. \n",
    "        # note: there is no way to check if a URL is of type 3 programatically; see discusson in thesis document\n",
    "\n",
    "    def GetTopDomain(this):\n",
    "        extract = tldextract.extract(this.URL.Hostname)\n",
    "        return extract.domain + '.' + extract.suffix\n",
    "\n",
    "\n",
    "    # returns all hyperlinks that point to inbound domains (relative paths = inbound)\n",
    "    def GetInboundURLs(this):\n",
    "        domain = this.URL.Hostname\n",
    "        URLs = this.Tree.xpath('(//a)[contains(@href, \"' + domain + '\") or not(contains(@href, \"http\"))]/@href')\n",
    "\n",
    "        urls_found = []\n",
    "        result     = []\n",
    "        for url in URLs:\n",
    "            try:\n",
    "                excluded = False\n",
    "                for exclude in this.Excludes:\n",
    "                    if exclude in url.lower():\n",
    "                        excluded = True\n",
    "                        break\n",
    "                if not excluded:\n",
    "                    if domain in url:\n",
    "                        url = BooterURL(url)\n",
    "                        url = url.Hostname + url.Path\n",
    "                        if url[len(url) - 1] == '/':\n",
    "                            url = url[:-1]\n",
    "                        if url not in urls_found:\n",
    "                            result.append(url)\n",
    "                    else:\n",
    "                        url = (this.RelativeURL + '/' + url).replace('//', '/')\n",
    "                        if url[len(url) - 1] == '/':\n",
    "                            url = url[:-1]\n",
    "                        if url not in urls_found:\n",
    "                            result.append(url)\n",
    "                    urls_found.append(url) # to check for duplicates\n",
    "            except Exception as ex:\n",
    "                pass\n",
    "\n",
    "        return result\n",
    "\n",
    "    # returns all outbound hyperlinks \n",
    "    def GetOutboundURLs(this):\n",
    "        domain = this.URL.Hostname\n",
    "        URLs = this.Tree.xpath('(//a)[contains(@href, \"http\") and not(contains(@href, \"' + domain + '\"))]/@href')\n",
    "\n",
    "\n",
    "        urls_found = []\n",
    "        result = []\n",
    "        for url in URLs:\n",
    "            excluded = False\n",
    "            for exclude in this.Excludes:\n",
    "                if exclude in url.lower():\n",
    "                    excluded = True\n",
    "                    break\n",
    "            if not excluded:\t\n",
    "                if domain in url:\n",
    "                    if url not in urls_found:\n",
    "                        result.append(url)\n",
    "                else:\t\t\n",
    "                    if url not in urls_found:\n",
    "                        result.append(url)\n",
    "                urls_found.append(url) # to check for duplicates\n",
    "        return result\n",
    "\n",
    "\n",
    "    # returns a tokenized list of words/phrases found in the crawled page's content\n",
    "    def GetContent(this):\n",
    "        text_content = []\n",
    "        if len(this.HTML) < 250000: # don't run xPath on too large HTML pages, takes ages (slightly bias-ed but otherwise destroys crawler times)\n",
    "            content      = this.Tree.xpath('(//p)[not(contains(@style, \"hidden\"))]/descendant-or-self::node()/text() | (//div)[not(contains(@style, \"hidden\"))]//descendant-or-self::node()[not(descendant-or-self::p) and not(descendant-or-self::script) and not (descendant-or-self::style)]/text()')\n",
    "            for text in content:\n",
    "                # first remove irelevant characters/symbols\n",
    "                remove = { '\\\\t', '\\\\r', '\\\\n', '&nbsp;' }\n",
    "                for removeable in remove:\n",
    "                    text = text.replace(removeable, '')\n",
    "                # then split text by whitespace and add each entry to text_content\n",
    "                text_content.append(text.split())\n",
    "\n",
    "            # merge all lists in text_content list into one final list of words\n",
    "            text_content = [item for sublist in text_content for item in sublist]\n",
    "        else:\n",
    "            # add bogus content to text_content (constant set as 1000)\n",
    "            for i in range(0, 1000):\n",
    "                text_content.append('too_large_html')\n",
    "\n",
    "        return text_content\n",
    "\n",
    "    # returns a bit-wise result whether this page contains an HTML login form (or register)\n",
    "    def HasLoginForm(this):\n",
    "        forms = this.Tree.xpath('(//form)//input[contains(@type, \"password\")]')\n",
    "        return len(forms) > 0\n",
    "\n",
    "\n",
    "    def __str__(this):\n",
    "        return this.URL.Full_URL;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.4\"><h2><a href=\"#TOC\">1.4. Crawler & in-depth Scaper</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import cfscrape\n",
    "import pythonwhois # https://github.com/joepie91/python-whois\n",
    "import signal\n",
    "from lxml import etree\n",
    "from colorama import Fore, Back, Style\n",
    "from random import choice, random\n",
    "from time import sleep\n",
    "from urlparse import urlparse\n",
    "\n",
    "# Simple callback class for timer management\n",
    "class timeout:\n",
    "    def __init__(self, seconds=1, error_message='Timeout'):\n",
    "        self.seconds = seconds\n",
    "        self.error_message = error_message\n",
    "    def handle_timeout(self, signum, frame):\n",
    "        raise TimeoutError(self.error_message)\n",
    "    def __enter__(self):\n",
    "        signal.signal(signal.SIGALRM, self.handle_timeout)\n",
    "        signal.alarm(self.seconds)\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.alarm(0)\n",
    "\n",
    "class Crawler:\n",
    "    'General purpose Crawler; hosts functionality relevant to crawling a '\n",
    "    'multitude of online web applications like forums, video-platforms and '\n",
    "    'social media. The Crawler is not operable by itself, but acts as a '\n",
    "    'superclass for specific crawler instances per web application.'\n",
    "    def __init__(this, target, sleep_level=1):\n",
    "        this.Target = target\n",
    "        this.Sleep_Level = sleep_level\n",
    "        this.URLs = []\n",
    "\n",
    "        # This is a list of domains that we EXCLUDE from our URL-potential-Booter analysis.\n",
    "        # We exclude these domains to avoid spending processing efforces as we know that \n",
    "        # subdomains of these domains CAN NOT be a Booter website. This list was composed based\n",
    "        # on our observations. Therefore it can/should/may be extended in the future!!!\n",
    "        this.Excludes = {\n",
    "            'dropbox.com',\n",
    "            'facebook.com',\n",
    "            'ge.tt',\n",
    "            'gyazo.com',\n",
    "            'github.com',\n",
    "            'hackforums.net',\n",
    "            'imgur.net',\n",
    "            'imgur.com',\n",
    "            'mediafire.com',\n",
    "            'prntscr.com',\n",
    "            'pastebin.com',\n",
    "            'rapidshare.com',\n",
    "            'sourceforge.net',\n",
    "            'twitter.com',\n",
    "            'uploading.com',\n",
    "            'urbandictionary.com',\n",
    "            'youtube.com',\n",
    "            'wikipedia',\n",
    "            'wiktionary',\n",
    "        }\n",
    "        # heuristic phrases to determine whether a website/domain is parked i.e. for sale\n",
    "        # - can be extended\n",
    "        this.ParkPhrases = {\n",
    "            \"this domain may be for sale\", \n",
    "            \"this domain is for sale\", \n",
    "            \"buy this domain\", \n",
    "            \"this web page is parked\", \n",
    "            \"this domain name expired on\", \n",
    "            \"backorder this domain\", \n",
    "            \"this domain is available through\",\n",
    "        }\n",
    "\n",
    "    # =========================================================================\n",
    "    # INITIALIZATION\n",
    "    # =========================================================================\n",
    "    # configures all connection objects and optionally logins into the service\n",
    "    def Initialize(this):\n",
    "        this.PrintUpdate('initiating crawling procedures')\n",
    "        this.PrintDivider()\n",
    "        this.Session = requests.Session()\n",
    "        # possible user-agent strings for the crawler system's 'user-agent' header flag\n",
    "        # a random user_agent string is selected each subsequent crawler run as to help\n",
    "        # avoid detection; manual update required from time to time\n",
    "        user_agents = [ \n",
    "            # 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',\n",
    "            # 'Opera/9.25 (Windows NT 5.1; U; en)',\n",
    "            # 'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "            # 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36', # this is currently enough for all purposes\n",
    "        ] \n",
    "        # http header\n",
    "        this.Header = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "            'Accept-Language' : 'nl-NL,nl;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Cache-Control' : 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'User-Agent': choice(user_agents),\n",
    "        }\n",
    "        this.PrintDebug(str(this.Header))\n",
    "\n",
    "\n",
    "    # Follows login procedures to initiate a session object per web application\n",
    "    def Login(this, url, succes_page, post_data={}):\n",
    "        this.PrintDivider()\n",
    "        this.PrintUpdate('attempting to login at: ' + url)\n",
    "        this.PrintDivider()\n",
    "\n",
    "        response = this.Session.post(url, data=post_data, headers=this.Header)\n",
    "        js_scraper = cfscrape.create_scraper()\n",
    "        # login process specific to hackforums.net; during the reseach hackforums.net enabled additional\n",
    "        # security checks at their login services. Continously requesting login attemps seems to bypass\n",
    "        # their detective measures. Can (and should) be generlized in the future.\n",
    "        redirect_count = 1\n",
    "        while 'HackForums.net has enabled additional security.' in response.text: \n",
    "            print('HackForums.net detection bypass -redirect.')\n",
    "            response = this.Session.post(url, data=post_data, headers=this.Header)\n",
    "            redirect_count = redirect_count + 1\n",
    "            if redirect_count == 5000: # increase to 5000, seemed to work last time?\n",
    "                break\n",
    "        if response.url == succes_page:\n",
    "            this.PrintUpdate('login succesful')\n",
    "            this.PrintDivider()\n",
    "            return True\n",
    "        else:\n",
    "            this.PrintError('failed to login to target server. Is the target online or blocked?')\n",
    "            return False\n",
    "\n",
    "    # adds a list of url substrings thst should be excluded from list of URLs\n",
    "    def AddExcludes(this, excludes):\n",
    "        this.Excludes = excludes\n",
    "\n",
    "    # =========================================================================\n",
    "    # CRAWLER: GENERATING A PBD LIST\n",
    "    # =========================================================================\n",
    "    # crawls the web service, should be overriden in each subclass\n",
    "    def Crawl(this, max_date):\n",
    "        this.PrintError('Crawler.Crawl function not instantiated!')\n",
    "\n",
    "    # determines whether the url should be excluded\n",
    "    def IsExcluded(this, URL):\n",
    "        for excluded in this.Excludes:\n",
    "            if excluded in URL.Full_URL:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # determines whether a url is already added to the URL list\n",
    "    def IsDuplicate(this, URL):\n",
    "        for i in range(len(this.URLs)):\n",
    "            if this.URLs[i].UniqueName() == URL.UniqueName():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # adds a URL to the final URL/PBD list if it meets all conditions\n",
    "    def AddToList(this, URL, source='?'):\n",
    "        if not this.IsDuplicate(URL) and not this.IsExcluded(URL):\n",
    "            try:\n",
    "                # get status/respnse-code and resolved-url of URL\n",
    "                status = this.GetStatus(URL.Full_URL)\n",
    "                URL    = BooterURL(status[2])\n",
    "                # save in database\n",
    "                SaveURL(URL, source, status[0])\n",
    "                # then save in list if proper response or post error\n",
    "                if status[1] == 200 or status[1] == 403 or status[1] == 202:\n",
    "                    BooterURL.Status = status # add status to URL for later use\n",
    "                    this.URLs.append(URL)\n",
    "                    this.PrintLine('ADDED TO BE SCRAPED: ' + URL.Full_URL, Fore.BLUE)\n",
    "                    if status[1] == 403 or status[1] == 202:\n",
    "                        print('Website blocked crawler; manually verify!')\n",
    "                else:\n",
    "                    this.PrintNote('incorrect response code [' + str(status[1]) + ']: ' + URL.Full_URL)\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: ' + str(ex))\n",
    "\n",
    "    # finish crawling and output URL/PBD list to file\n",
    "    def Finish(this, output_file):\n",
    "        this.PrintUpdate('writing output to file \\'' + output_file + '\\'')\n",
    "        f = open(output_file, 'w')\n",
    "        for URL in this.URLs:\n",
    "            f.write(URL.UniqueName() + '\\n')\n",
    "        this.PrintUpdate('FINISHED; closing operations')\n",
    "        this.PrintDivider()\n",
    "        return this.URLs\n",
    "\n",
    "    # sleeps for a semi-random amount of time to mitigate bot detection\n",
    "    def Sleep(this):\n",
    "        # semi-randomly vary sleep amount to emulate human behavior\n",
    "        sleep(this.Sleep_Level + random() * this.Sleep_Level) \n",
    "\n",
    "    # enables javascript to circumvent JS bot detection (GET the actual Web Page)\n",
    "    def JSCrawl(this, url):\n",
    "        js_scraper = cfscrape.create_scraper()\n",
    "        response = js_scraper.get(url, headers=this.Header, timeout=15.0) \n",
    "        redirect_count = 0\n",
    "        while 'http-equiv=\"Refresh\"' in response.text: \n",
    "            response = js_scraper.get(response.url, headers=this.Header, timeout=15.0) \n",
    "            redirect_count = redirect_count + 1\n",
    "            if redirect_count == 5:\n",
    "                this.PrintError('REDIRECT COUNT OF 5 REACHED! ' + response.url)\n",
    "                break\n",
    "        return response\n",
    "\n",
    "    # retrieves status of website: resolves URL, response code and whether it\n",
    "    # is offline/online or for sale\n",
    "    def GetStatus(this, url):\n",
    "        response = this.JSCrawl(url)\n",
    "        if response.status_code == 200 or response.status_code == 403 or response.status_code == 202:\n",
    "            # check if for sale, otherwise site deemed as online\n",
    "            for phrase in this.ParkPhrases:\n",
    "                if phrase in response.text.lower():\n",
    "                    return ('free', response.status_code, response.url)\n",
    "            # else site is online\n",
    "            return ('on', response.status_code, response.url)\t\t\t\t\n",
    "        else:\n",
    "            return ('off', response.status_code, response.url)\n",
    "    # =========================================================================\n",
    "    # SCRAPER: EVIDENCE AND HEURISTICS\n",
    "    # =========================================================================\n",
    "    # scrapes a (potential) Booter URL for evidence as reported by Booter \n",
    "    # characteristics in 'The Generation of Booter (black)lists'\n",
    "    def Scrape(this, URL, days_update=0):\n",
    "        # check if number of days_update days have passed since last update, and if so, update\n",
    "        if RowExists('scores', URL.UniqueName()): \n",
    "            last_update  = GetSingleValue('scores', URL.UniqueName(), 'lastUpdate')\n",
    "            last_update  = datetime.datetime.strptime(last_update, '%Y-%m-%d %H:%M:%S')\n",
    "            current_date = datetime.datetime.now()\n",
    "            difference   = (current_date - last_update).days\n",
    "            if difference < days_update:\n",
    "                this.PrintDivider()\n",
    "                this.PrintDebug('Skip scrape: ' + URL.UniqueName() + '; last scraped: ' + str(last_update))\n",
    "                return \n",
    "        # else, start scraping\n",
    "        try:\n",
    "            this.PrintDivider()\n",
    "            this.PrintDebug('STARTING SCRAPE: ' + URL.Full_URL)\n",
    "            ### 1. structure characteristics\n",
    "            this.PrintDivider()\n",
    "            this.PrintUpdate('obtaining structure-based characteristics')\n",
    "            this.PrintDivider()\n",
    "            # - 1.1 number of pages\n",
    "            number_of_pages \t= -1.0\n",
    "            number_of_pages_raw = -1.0 # also store a raw score for data analysis \n",
    "            # crawl through the URL and each subsequent inbound URL \n",
    "            crawled     = []\n",
    "            crawl_count = 0\n",
    "            max_urls    = 50\n",
    "            # - get landing page\n",
    "            landing_page = CrawlPage(this.JSCrawl(URL.Full_URL))\n",
    "            inbounds     = landing_page.GetInboundURLs()\n",
    "            crash_pages  = []\n",
    "\n",
    "            this.PrintNote('scraping: ' + URL.Full_URL)\t\n",
    "            page_url\t = (landing_page.URL.Hostname + landing_page.URL.Path).replace('//','/')\n",
    "            if page_url[len(page_url) - 1] == '/':\n",
    "                page_url = page_url[:-1]\n",
    "            depth_levels = { page_url : 0 }\n",
    "            inbounds.append(page_url)\n",
    "\n",
    "            # 1.1.1 store depth levels of landing page found inbound urls\n",
    "            for inbound in inbounds:\n",
    "                if inbound not in URL.Full_URL and inbound not in page_url:\n",
    "                    depth_levels[inbound] = 1\n",
    "\n",
    "            crawled.append(landing_page)\n",
    "\n",
    "            # 1.1.2. then from each found (inbound) URL, keep crawling until maximum crawl limit is reached\n",
    "            fail_loop_attempts = 0\t\t\n",
    "            while crawl_count < len(inbounds) and crawl_count < max_urls - 1:\n",
    "                inbound = inbounds[crawl_count]\n",
    "                if inbound not in crawled and inbound not in crash_pages:\n",
    "                    try:\n",
    "                        with timeout(seconds=20):\n",
    "                            # crawl next page and obtain new inbound/outbound urls\n",
    "                            this.PrintNote('scraping: ' + 'http://' + inbound)\n",
    "\n",
    "                            response = this.JSCrawl('http://' + inbound)\n",
    "                            crawl_page    = CrawlPage(response)\n",
    "                            new_inbounds  = crawl_page.GetInboundURLs()\n",
    "                            # for each of the new_inbounds, set their depth level if less than currently stored (or not yet stored)\n",
    "                            for new_inbound in new_inbounds:\n",
    "                                if new_inbound not in depth_levels:\n",
    "                                    depth_levels[new_inbound] = depth_levels[inbound] + 1\n",
    "                            # merge results\n",
    "                            inbounds  = inbounds  + list(set(new_inbounds)  - set(inbounds))\n",
    "                            # then continue\n",
    "                            crawled.append(crawl_page)\n",
    "                            crawl_count = crawl_count + 1\n",
    "                    except Exception as ex:\n",
    "                        this.PrintError('EXCEPTION: ' + str(ex))\n",
    "                        crash_pages.append(inbound)\n",
    "                        crawl_count = crawl_count + 1\n",
    "                else:\n",
    "                    fail_loop_attempts += 1 # aborts loop after 10000 tries; indicating infinite loop\n",
    "                    if fail_loop_attempts > 10000:\n",
    "                        this.PrintError('INFINITE LOOP detected; aborting scrape')\n",
    "                        break\n",
    "\n",
    "\n",
    "            # 1.1.3 calculate scores\n",
    "            # use quadratic equation (to give numbers with low pages higher scores)\n",
    "            # equation: y = -2x^2 + 1 ... (y=0) = 0.707\n",
    "            # with 25 pages, score is 0.75, so first half pages give 1/4 drop-down in score\n",
    "            number_of_pages_raw = len(inbounds)\n",
    "            if number_of_pages_raw > 0:\n",
    "                number_of_pages = -2 * (number_of_pages_raw / (50/0.707106781\t)) ** 2 + 1\n",
    "                number_of_pages = max(number_of_pages, 0.0) \n",
    "            this.PrintUpdate('number of pages: ' + str(number_of_pages_raw))\n",
    "\n",
    "            # - 1.2. URL type\n",
    "            # how to determine its url type? difficult/impossible to determine programmaticaly \t\n",
    "            url_type_raw = landing_page.URLType()\n",
    "            if url_type_raw == 2:\n",
    "                url_type = 0.0\n",
    "            else:\n",
    "                url_type = 1.0\n",
    "            this.PrintUpdate('url type: ' + str(url_type_raw))\n",
    "\n",
    "            # - 1.3. Average depth level\n",
    "            # take previously retrieved depth levels and take average\n",
    "            average_depth_level     = 0.0\n",
    "            average_depth_level_raw = 0.0\n",
    "            for depth_url in depth_levels:\n",
    "                average_depth_level_raw = average_depth_level_raw + depth_levels[depth_url]\n",
    "            # calculate score: take linear value between 1.0 and 3.0\n",
    "            average_depth_level_raw = average_depth_level_raw / len(depth_levels)\n",
    "            if average_depth_level_raw <= 1.0:\n",
    "                average_depth_level = 1.0\n",
    "            else:\n",
    "                average_depth_level = max(1.0 - ((average_depth_level_raw - 1.0) / 2.0), 0.0)\n",
    "            this.PrintUpdate('average depth level: ' + str(average_depth_level_raw))\n",
    "\n",
    "            # - 1.4. Average URL length\n",
    "            average_url_length     = -1.0\n",
    "            average_url_length_raw = -1.0\n",
    "            for page in inbounds: # use inbounds, not pages crawled as they give much more results\n",
    "                average_url_length_raw = average_url_length_raw + len(page)\n",
    "            # calculate score: interpolate linearly from lowest occurence to highest Booter occurence\n",
    "            average_url_length_raw = average_url_length_raw / len(inbounds) \n",
    "            if average_url_length_raw <= 15:\n",
    "                average_url_length = 1.0\n",
    "            else:\n",
    "                average_url_length = max(1.0 - ((average_url_length_raw - 15) / 15), 0.0)\n",
    "            this.PrintUpdate('average url length: ' + str(average_url_length_raw))\n",
    "\n",
    "\n",
    "            ### 2. content-based characteristics\n",
    "            this.PrintDivider()\n",
    "            this.PrintUpdate('obtaining content-based characteristics')\n",
    "            this.PrintDivider()\n",
    "\n",
    "            # get whois information\n",
    "            # \"Each part represents the response from a specific WHOIS server. Because the WHOIS doesn't force WHOIS \n",
    "            # servers to follow a unique response layout, each server needs its own dedicated parser.\"\n",
    "            domain_age                      = -1.0\n",
    "            domain_age_raw                  = -1.0\n",
    "            domain_reservation_duration     = -1.0\n",
    "            domain_reservation_duration_raw = -1.0\n",
    "            try:\n",
    "                with timeout(seconds=10):\n",
    "                    whois = pythonwhois.get_whois(landing_page.GetTopDomain(), False) # http://cryto.net/pythonwhois/usage.html\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: get WHOIS data: ' + str(ex))\n",
    "            try:\n",
    "                # - 2.1. Domain age\n",
    "                current_date    = datetime.datetime.today()\n",
    "                date_registered = whois['creation_date'][0]\n",
    "                domain_age_raw\t= (current_date - date_registered).days\n",
    "                # calculate score: linear interpolation between current_date and first occurence of \n",
    "                # booter in data: 2011\n",
    "                days_since_first = (current_date - datetime.datetime(2011, 10, 28)).days\n",
    "                domain_age = max(1.0 - (domain_age_raw / days_since_first), 0.0)\n",
    "                this.PrintUpdate('domain age: ' + str(domain_age_raw))\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: whois keywords, likely registrar: ' + str(ex))\n",
    "\n",
    "            try:\n",
    "                # - 2.2 Domain reservation duration\n",
    "                current_date  \t\t\t\t    = datetime.datetime.today()\n",
    "                expire_date    \t\t\t        = whois['expiration_date'][0]\n",
    "                domain_reservation_duration_raw = (expire_date - current_date).days\n",
    "                # calculate score: between 1 - 2 years; < 1 year = 1.0\n",
    "                if domain_reservation_duration_raw < 183:\n",
    "                    domain_reservation_duration = 1.0\n",
    "                else:\n",
    "                    # domain_reservation_duration = max(1.0 - ((domain_reservation_duration_raw - 365) / 365), 0.0)\n",
    "                    domain_reservation_duration = max(1.0 - (domain_reservation_duration_raw - 183) / 182, 0.0)\n",
    "                this.PrintUpdate('domain reservation duration: ' + str(domain_reservation_duration_raw))\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: whois keywords, likely registrar: ' + str(ex))\n",
    "\n",
    "            # - 2.3. WHOIS private\n",
    "            # there doesn't exist a private WHOIS field, but private information can be obtained through\n",
    "            # heuristics using common phrases found by privacy-replacing registry information.\n",
    "            try:\n",
    "                private_phrases = [\n",
    "                    'whoisguard',\n",
    "                    'whoisprotect',\n",
    "                    'domainsbyproxy',\n",
    "                    # 'whoisprivacyprotect', # are caught by privacy term anyways\n",
    "                    'protecteddomainservices',\n",
    "                    # 'myprivacy',\n",
    "                    # 'whoisprivacycorp',\n",
    "                    # 'privacyprotect',\n",
    "                    'namecheap',\n",
    "                    'privacy',\n",
    "                    'private',\n",
    "                ]\n",
    "                whois_private = 0.0\n",
    "                reg_name \t  = whois['contacts']['registrant']['name'].lower()\n",
    "                reg_email \t  = whois['contacts']['registrant']['email'].lower()\n",
    "                for phrase in private_phrases:\n",
    "                    if phrase in reg_name or phrase in reg_email: #or phrase in reg_org :\n",
    "                        whois_private = 1.0\n",
    "                        break\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: whois keyfields, private set to -1.0: ' + str(ex))\n",
    "                whois_private = -1.0\n",
    "            this.PrintUpdate('WHOIS private: ' + str(whois_private))\n",
    "\n",
    "            # - 2.4. DPS\n",
    "            # similar to whois private, use heuristics to determine whether website uses DPS,\n",
    "            # first we try to determine whether it uses DNS based DPS by checking nameservers\n",
    "            try:\n",
    "                with timeout(seconds=10):\n",
    "                    dps_names = [\n",
    "                        'cloudflare',\n",
    "                        'incapsula',\n",
    "                        'prolexic',\n",
    "                        'akamai',\n",
    "                        'verisign',\n",
    "                        'blazingfast',\n",
    "                    ]\n",
    "                    dps = 0.0\n",
    "                    if 'nameservers' in whois:\n",
    "                        for nameserver in whois['nameservers']:\n",
    "                            if dps == 0.0:\n",
    "                                for dps_name in dps_names:\n",
    "                                    if dps_name in nameserver.lower():\n",
    "                                        dps = 1.0\n",
    "                                        break\n",
    "                    # if nothing found from nameservers, also check redirection history if dps redirect page was used\n",
    "                    if dps < 0.5:\n",
    "                        response_text = this.Session.post(URL.Full_URL, headers=this.Header, allow_redirects=False).text\n",
    "                        this.PrintNote('No DPS detected from NS; checking re-direction history')\n",
    "                        for dps_name in dps_names:\n",
    "                            if dps_name in response_text:\n",
    "                                dps = 1.0\n",
    "            except Exception as ex:\n",
    "                this.PrintError('EXCEPTION: dps set to -1.0: ' + str(ex))\n",
    "                dps = -1.0\n",
    "            this.PrintUpdate('DPS: ' + str(dps))\n",
    "\n",
    "            # - 2.5. Page rank\n",
    "            try:\n",
    "                url           = 'http://data.alexa.com/data?cli=10&dat=s&url=' + URL.Hostname\n",
    "                response      = this.Session.get(url)\n",
    "                tree \t      = etree.XML(response.text.encode('utf-8'))\n",
    "                page_rank_raw = tree.xpath('(//REACH)/@RANK')[0]\n",
    "                page_rank     = 0.0\n",
    "                if int(page_rank_raw) > 200000: # determined from highest booter (ipstresser.com - vdos-s.com) minus offset\n",
    "                    page_rank = 1.0\n",
    "            except Exception as ex:\n",
    "                page_rank_raw = 25426978.0 # set to highest occuring page rank (lower than that if non-existent)\n",
    "                page_rank = 1.0\n",
    "\n",
    "            this.PrintUpdate('Page rank: ' + str(page_rank_raw))\n",
    "\n",
    "\n",
    "            ### 3. host-based characteristics\n",
    "            this.PrintDivider()\n",
    "            this.PrintUpdate('obtaining host-based characteristics')\n",
    "            this.PrintDivider()\n",
    "\n",
    "            # - 3.1. Average content size\n",
    "            average_content_size     = 0.0\n",
    "            average_content_size_raw = 0.0\n",
    "            crawl_contents = []\n",
    "            for crawl_page in crawled:\n",
    "                crawl_content = crawl_page.GetContent();\n",
    "                crawl_contents.append(crawl_content)\n",
    "                average_content_size_raw = average_content_size_raw + len(crawl_content)\n",
    "            average_content_size_raw = average_content_size_raw / len(crawled)\n",
    "            # calculte score: linear interpolation between 50 - (avg_max_booter = 250)\n",
    "            if average_content_size_raw < 50:\n",
    "                average_content_size = 1.0\n",
    "            else:\n",
    "                average_content_size = max(1.0 - (average_content_size_raw - 50) / 200, 0.0)\n",
    "            this.PrintUpdate('Average content size: ' + str(average_content_size_raw))\n",
    "\n",
    "            # - 3.2. Outbound hyperlinks\n",
    "            outbound_hyperlinks     = 0.0\n",
    "            outbound_hyperlinks_raw = 0.0\n",
    "            for crawl_page in crawled:\n",
    "                outbound_hyperlinks_raw = outbound_hyperlinks_raw + len(crawl_page.GetOutboundURLs())\n",
    "            outbound_hyperlinks_raw = outbound_hyperlinks_raw / len(crawled)\n",
    "            # calculate score: linear interpolation between 0 and 2\n",
    "            outbound_hyperlinks = max(1.0 - outbound_hyperlinks_raw / 2.0, 0.0)\n",
    "            this.PrintUpdate('Average outbound hyperlinks: ' + str(outbound_hyperlinks_raw))\n",
    "\n",
    "            # - 3.3. Category-specific dictionary\n",
    "            dictionary = [ 'stress', 'booter', 'ddos', 'powerful', 'resolver', 'price' ] # or pric, so we can also get items like pricing\n",
    "            category_specific_dictionary     = 0.0\n",
    "            category_specific_dictionary_raw = 0.0\n",
    "            words = landing_page.GetContent()\n",
    "            for item in dictionary:\n",
    "                for word in words:\n",
    "                    if item in word.lower():\n",
    "                        category_specific_dictionary_raw = category_specific_dictionary_raw + 1\n",
    "            # - now calculate percentage of these words occuring relative to total page content\n",
    "            if len(words) > 0:\n",
    "                category_specific_dictionary_raw = category_specific_dictionary_raw / len(words)\n",
    "            else:\n",
    "                category_specific_dictionary_raw = 0.0\n",
    "            # calculate score: interpolate between 0.01 and 0.05             \n",
    "            category_specific_dictionary = max(1.0 - ((category_specific_dictionary_raw - 0.01) / 0.04), 0.0)\n",
    "            this.PrintUpdate('Category specific dictionary: ' + str(category_specific_dictionary_raw))\n",
    "\n",
    "            # - 3.4. Resolver indication (only the landing page); perhaps extend to all pages in future version?\n",
    "            resolver_indication = 0.0\n",
    "            dictionary = [ 'skype' , 'xbox', 'resolve', 'cloudflare' ]\n",
    "            for item in dictionary:\n",
    "                for word in words:\n",
    "                    if item in word.lower():\n",
    "                        resolver_indication = 1.0\n",
    "            this.PrintUpdate('Resolver indication: ' + str(resolver_indication))\n",
    "\n",
    "            # - 3.5. Terms of Services page\n",
    "            terms_of_services_page = 0.0\n",
    "            # - check if one of the urls contains tos or terms and service\n",
    "            for url in inbounds:\n",
    "                url = url.lower();\n",
    "                if '/tos' in url or 'terms' in url and 'service' in url:\n",
    "                    terms_of_services_page = 1.0\n",
    "            # - if not yet found, also check for content hints in all the pages\n",
    "            tos_phrases = [\n",
    "                'terms and conditions', \n",
    "                'purposes intended', \n",
    "                'you are responsible', \n",
    "                'we have the right', \n",
    "                'terms of service',\n",
    "                'understand and agree',\n",
    "            ]\n",
    "            if terms_of_services_page < 0.5:\n",
    "                for content in crawl_contents:\n",
    "                    text = ' '.join(content).lower()\n",
    "                    for phrase in tos_phrases:\n",
    "                        if phrase in text:\n",
    "                            terms_of_services_page = 1.0\n",
    "                            break\n",
    "                    if terms_of_services_page > 0.5:\n",
    "                        break\n",
    "            this.PrintUpdate('Terms of services page: ' + str(terms_of_services_page))\n",
    "\n",
    "            # - 3.6. Login-form depth level\n",
    "            # this does also take into account register forms, but that's generally expected\n",
    "            # to be on the same level as login forms so not an issue\n",
    "            login_form_depth_level     = -1.0\n",
    "            login_form_depth_level_raw =  3.0 # set to max found in dataset if non-existent\n",
    "            forms_urls = []\n",
    "            for page in crawled:\n",
    "                if page.HasLoginForm(): \n",
    "                    page_url = page.URL.Hostname + page.URL.Path + page.URL.Query\n",
    "                    if page_url[len(page_url) - 1] == '/':\n",
    "                        page_url = page_url[:-1]\n",
    "                    forms_urls.append(page_url)\n",
    "            min_depth = 100\n",
    "            for url in forms_urls:\n",
    "                for depth_url in depth_levels:\n",
    "                    if depth_url == url:\n",
    "                        if depth_levels[url] < min_depth:\n",
    "                            min_depth = depth_levels[url]\n",
    "                        break\n",
    "            if min_depth != 100:\n",
    "                login_form_depth_level_raw = min_depth\n",
    "            # transform to score (if depth level exceeds 2, score becomes 0)\n",
    "            login_form_depth_level = min(max(1.0 - min_depth * 0.5, 0.0), 1.0)\n",
    "            this.PrintUpdate('Login-form depth level: ' + str(login_form_depth_level_raw))\n",
    "\n",
    "            ### 4. Now save the results into the database\n",
    "            SaveScore('scores',\n",
    "                URL,\n",
    "                datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                number_of_pages,\n",
    "                url_type,\n",
    "                average_depth_level,\n",
    "                average_url_length,\n",
    "                domain_age,\n",
    "                domain_reservation_duration,\n",
    "                whois_private,\n",
    "                dps,\n",
    "                page_rank,\n",
    "                average_content_size,\n",
    "                outbound_hyperlinks,\n",
    "                category_specific_dictionary,\n",
    "                resolver_indication,\n",
    "                terms_of_services_page,\n",
    "                login_form_depth_level\n",
    "            )\n",
    "            # also store raw feature data for analysis\n",
    "            SaveScore('characteristics',\n",
    "                URL,\n",
    "                datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                number_of_pages_raw,\n",
    "                url_type_raw,\n",
    "                average_depth_level_raw,\n",
    "                average_url_length_raw,\n",
    "                domain_age_raw,\n",
    "                domain_reservation_duration_raw,\n",
    "                whois_private,\n",
    "                dps,\n",
    "                page_rank_raw,\n",
    "                average_content_size_raw,\n",
    "                outbound_hyperlinks_raw,\n",
    "                category_specific_dictionary_raw,\n",
    "                resolver_indication,\n",
    "                terms_of_services_page,\n",
    "                login_form_depth_level_raw\n",
    "            )\n",
    "            this.Sleep()\n",
    "\n",
    "        except Exception as ex:\n",
    "            this.PrintError('EXCEPTION: Scrape failed: connection host ' + str(ex))\n",
    "            # raise\n",
    "    # =========================================================================\n",
    "    # PRINT FUNCTIONALITY\n",
    "    # =========================================================================\n",
    "    def PrintLine(this, text, color=Fore.WHITE, style=Style.NORMAL):\n",
    "        print(color + style + text[:80] + Style.RESET_ALL)\n",
    "    def PrintDivider(this):\n",
    "        this.PrintLine('................................................................................', Fore.YELLOW)\n",
    "    def PrintUpdate(this, text):\n",
    "        this.PrintLine('SCRAPPED: ' + text, Fore.GREEN)\n",
    "    def PrintError(this, text):\n",
    "        this.PrintLine('[ERROR] ' + text, Fore.RED)\n",
    "    def PrintDebug(this, text):\n",
    "        this.PrintLine('[DEBUG] ' + text, Fore.CYAN)\n",
    "    def PrintNote(this, text):\n",
    "        this.PrintLine('[NOTE] ' + text, Fore.WHITE, Style.DIM)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.5\"><h1><a href=\"#TOC\">1.5. Refining our Crawler/Scraper</a></h1></div>\n",
    "<div id=\"1.5.1\"><h2><a href=\"#TOC\">1.5.1. Refining our Crawler applied to Google</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from colorama import Fore, Back, Style\n",
    "import json\n",
    "\n",
    "# documentation: https://developers.google.com/web-search/docs/#The_Basics\n",
    "\n",
    "class Crawler_Google2(Crawler):\n",
    "    'Crawler of Google via default web requests'\n",
    "    def __init__(this, sleep_level=1):\n",
    "        domain = 'https://www.google.com/search?ie=UTF-8&num=100' \n",
    "        Crawler.__init__(this, domain, sleep_level)\n",
    "        this.PrintNote('CRAWLING GOOGLE')\n",
    "        this.PrintDivider()\n",
    "        this.Initialize()\n",
    "        # this.Header['referer'] = 'utwente.nl'\n",
    "\n",
    "    # def Login(this):\n",
    "        # no login\n",
    "        # this.PrintError('NO LOGIN REQUIRED')\n",
    "\n",
    "    # overrides Crawler's crawl function\n",
    "    def Crawl(this, max_results=100):\n",
    "        keywords = ['Booter', 'DDOSer', 'Stresser']\n",
    "        \n",
    "        nr_pages = int(max_results / 100)\n",
    "        this.PrintUpdate('initiating crawling procedures: Google')\n",
    "\n",
    "        for keyword in keywords:\n",
    "            this.PrintDivider()\n",
    "            this.PrintNote('KEYWORD: ' + keyword)\n",
    "            this.PrintDivider()\n",
    "            for i in range(0, nr_pages): \n",
    "                counter = 0\n",
    "                # dynamically generate search query\n",
    "                query =  \"&q=\" + keyword+ '&start=' + str(i * 100) + '&filter=0' \n",
    "                url = this.Target + query\n",
    "\n",
    "                # read html and parse JSON\n",
    "                response = this.JSCrawl(url)                 \n",
    "                tree = html.fromstring(response.text) \n",
    "\n",
    "                urls = tree.xpath('(//div)[@class=\"g\"]//h3[@class=\"r\"]/a/@href')\n",
    "\n",
    "                split   = 10\n",
    "                for url in urls:\n",
    "                    this.PrintNote(str(counter)+') '+url) #jjsantanna Debugging\n",
    "                    try:\n",
    "                        # parse url\n",
    "                        if '/url?q=' in url:\n",
    "                            url = url[7:].split('&sa')[0]\n",
    "                        this.AddToList(BooterURL(url), 'Google')\n",
    "\n",
    "                        if counter % split == 0:\n",
    "                            this.PrintDivider()\n",
    "                        counter = counter + 1\n",
    "                    except Exception as ex:\n",
    "                        this.PrintError('EXCEPTION: ' + str(ex))\n",
    "                this.Sleep()\n",
    "\n",
    "\n",
    "        this.PrintNote('DONE; '+ str(counter)+' found, but only ' + str(len(this.URLs)) + ' potential Booters')\n",
    "        this.PrintDivider()\n",
    "        \n",
    "\n",
    "crawler = Crawler_Google2(1)\n",
    "crawler.Crawl(100) # up to ~500 results (more is not possible by Google)\n",
    "# results_google = crawler.Finish('crawl_google2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.5.2\"><h2><a href=\"#TOC\">1.5.2. Refining our Crawler applied to Youtube</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from lxml import html\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "class Crawler_Youtube(Crawler):\n",
    "    'Crawler of Youtube via default web requests'\n",
    "    def __init__(this, sleep_level=1):\n",
    "        domain = 'https://www.youtube.com/results?' \n",
    "        Crawler.__init__(this, domain, sleep_level)\n",
    "\n",
    "        this.PrintNote('CRAWLING YOUTUBE')\n",
    "        this.PrintDivider()\n",
    "        this.Initialize()\n",
    "\n",
    "    # overrides Crawler's crawl function\n",
    "    def Crawl(this, max_results=100):\n",
    "        keywords = ['booter', 'stresser', 'ddoser']\n",
    "        nr_pages = int(max_results / 10)\n",
    "        \n",
    "        this.PrintUpdate('initiating crawling procedures: Youtube')\n",
    "\n",
    "        for keyword in keywords:\n",
    "            this.PrintDivider()\n",
    "            this.PrintNote('KEYWORD: ' + keyword)\n",
    "            for i in range(0, nr_pages): \n",
    "                counter = 0\n",
    "                try:\n",
    "                    # dynamically generate search query\n",
    "                    query =  '&search_query=\"' + keyword\t+ '\"&page=' + str(i)\n",
    "                    url = this.Target + query\n",
    "                    this.PrintDivider()\n",
    "                    this.PrintDebug('crawling: ' + query) \n",
    "                    # read html and parse JSON\n",
    "                    response = this.JSCrawl(url)\n",
    "                    tree \t = html.fromstring(response.text) \n",
    "                    split   = 10\n",
    "\n",
    "                    urls_found = []\n",
    "\n",
    "                    descriptions = tree.xpath('(//div)[contains(@class, \"yt-lockup-description\")]/descendant-or-self::*/text()')\t\t\t\t\t\n",
    "                    \n",
    "                    for description in descriptions:\n",
    "                        # check whether description certainly doesn't hold an online booter\n",
    "                        if this.StopSearching(description):\n",
    "                            continue\n",
    "\n",
    "                        # find all urls in description\n",
    "                        urls = re.findall('http[s]?:\\/\\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', description)\n",
    "                        for url in urls:\n",
    "                            urls_found.append(url)\n",
    "\n",
    "\n",
    "                    # also check for explicit urls in descriptions\n",
    "                    urls = tree.xpath('(//div)[contains(@class, \"yt-lockup-description\")]//a/@href')\t\t\t\t\t\n",
    "                    for url in urls:\n",
    "                        this.PrintNote('Found: '+str(url)) ## Debuging jjsantanna\n",
    "                        if url not in urls_found:\n",
    "                            urls_found.append(url)\n",
    "                            \n",
    "\n",
    "                    this.PrintDivider()\n",
    "                    this.PrintUpdate('obtained ' + str(len(urls_found)) + ' potential URLs; extracting...')\n",
    "                    this.PrintDivider()\n",
    "\n",
    "                    # resolve each url and add returned url to final urls\n",
    "                    for url in urls_found:\n",
    "                        this.AddToList(BooterURL(url), 'Youtube')\n",
    "                        counter = counter + 1\n",
    "                        if counter % split == 0:\n",
    "                            this.PrintDivider()\t\n",
    "\n",
    "                    this.Sleep()\n",
    "                except Exception as ex:\n",
    "                    this.PrintError('EXCEPTION: ' + str(ex))\n",
    "                    this.Sleep()\n",
    "\n",
    "        this.PrintUpdate('DONE; found ' + str(len(this.URLs)) + ' potential Booters')\n",
    "        this.PrintDivider()\n",
    "\n",
    "\n",
    "    # decides to stop crawling a specific description as soon as\n",
    "    # certain stop keywords are found like 'tutorial'\n",
    "    def StopSearching(this, description):\n",
    "        stop_words = {\n",
    "            'tutorial'\n",
    "            'download:'\n",
    "            'gui',\n",
    "            'dekstop'\n",
    "        }\n",
    "        text = description.lower()\n",
    "        for word in stop_words:\n",
    "            if word in text:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "#################    \n",
    "# USAGE EXAMPLE #\n",
    "#################\n",
    "crawler = Crawler_Youtube(1)\n",
    "crawler.Crawl(100) # up to ~500 results (similar limit to Google)\n",
    "crawler.Finish('crawl_youtube.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1.5.3\"><h2><a href=\"#TOC\">1.5.3. Refining our Crawler applied to Hackerforums.net</a></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "class Crawler_Hackforums(Crawler):\n",
    "    'Crawler for web services of www.hackforums.net'\n",
    "    def __init__(self, sleep_level=1):\n",
    "        domain = 'http://www.hackforums.net/' \n",
    "        Crawler.__init__(self, domain, sleep_level)\n",
    "\n",
    "        self.PrintNote('CRAWLING HACKFORUMS')\n",
    "        self.PrintDivider()\n",
    "        self.Initialize()\n",
    "\n",
    "    # Login to hackforums.net\n",
    "    # NOTE: sometimes their login procedures block automated attempts; in that case a semi-brute\n",
    "    # force attempt is executed in the Crawler superclass.\n",
    "    def Login(self):\n",
    "        username = 'ADD YOUR USERNAME HERE!!!' \n",
    "        password = 'ADD YOUR PASSWORD HERE!!!'\n",
    "        url = self.Target + 'member.php'\n",
    "        post_data = {\n",
    "            'username': username,\n",
    "            'password': password,\n",
    "            'action': 'do_login',\n",
    "            'url': 'http://www.hackforums.net/index.php',\n",
    "            'my_post_key': '60aae6001602ef2e0bd45033d53f53dd'\n",
    "        }\n",
    "        return super(Crawler_Hackforums, self).Login(\n",
    "            url, \n",
    "            self.Target + 'index.php', \n",
    "            post_data\n",
    "        )\n",
    "\n",
    "    # overrides Crawler's crawl function\n",
    "    def Crawl(self, max_date):\n",
    "        # crawl of hackforums.net is executed in three steps:\n",
    "        # 1. we retrieve all interesting forum posts\n",
    "        # 2. we extract potential Booter URLs from these posts\n",
    "        # 3. collect all related evidence and calculate scores \n",
    "        #    for later evaluation\n",
    "        target_url = self.Target + 'forumdisplay.php?fid=232&page='\n",
    "        ### step 1. retrieve all relevant forum posts\n",
    "        forum_items  \t = []\n",
    "        current_page \t = 1\n",
    "        max_pages \t \t = 1\n",
    "        max_date_reached = False\n",
    "        self.PrintUpdate('initiating crawling procedures: HackForums')\n",
    "        self.PrintDivider()\n",
    "\n",
    "        # crawl first forum page and parse into XML tree\n",
    "        response = self.Session.post(target_url + str(current_page), headers=self.Header)\n",
    "        tree \t = html.fromstring(response.text) \n",
    "\n",
    "        # analyze structure and get relevant properties (via XPATH)\n",
    "        self.PrintUpdate('analyzing structure and retrieving Booter candidates')\n",
    "        self.PrintDivider()\n",
    "        max_pages = int(tree.xpath('//a[@class=\"pagination_last\"]/text()')[0])\n",
    "        # max_pages = 1 # for debug\n",
    "        # now start crawling\n",
    "        while current_page <= max_pages and not max_date_reached:\n",
    "            self.PrintUpdate('crawling page ' + str(current_page) + '/' + str(max_pages))\n",
    "            self.PrintDivider()\n",
    "            # get forum items\n",
    "            forum_titles = tree.xpath('//td[contains(@class,\"forumdisplay_\")]/div/span[1]//a[contains(@class, \" subject_\")]/text()')\n",
    "            forum_urls   = tree.xpath('//td[contains(@class,\"forumdisplay_\")]/div/span[1]//a[contains(@class, \" subject_\")]/@href')\n",
    "            forum_dates  = tree.xpath('//td[contains(@class,\"forumdisplay_\")]/span/text()[1]')\n",
    "            # get data of each forum item\n",
    "            for i in range(len(forum_titles)):\n",
    "                item = ForumItem(forum_titles[i], self.Target + forum_urls[i], forum_dates[i])\n",
    "                if item.IsPotentialBooter():\n",
    "                    forum_items.append(item)\n",
    "                    print(item)\n",
    "                # check if max date is reached\n",
    "                if item.Date < max_date:\n",
    "                    max_date_reached = True\n",
    "                    self.PrintDivider()\n",
    "                    self.PrintUpdate('date limit reached; aborting...')\n",
    "                    self.PrintDivider()\n",
    "                    break\n",
    "            # print a divider after each forum page\n",
    "            self.PrintDivider()\n",
    "            # get url of next page and re-iterate\n",
    "            current_page = current_page + 1\n",
    "            next_url     = target_url + str(current_page)\n",
    "            response     = self.Session.post(next_url, headers=self.Header)\n",
    "            tree         = html.fromstring(response.text)            \n",
    "\n",
    "            if current_page <= max_pages:\n",
    "                self.Sleep()\n",
    "        # forum crawling is complete, print (sub)results\n",
    "        self.PrintUpdate('items found: ' + str(len(forum_items)))\n",
    "        self.PrintDivider()\n",
    "\n",
    "        ### step 2. extract potential Booters from target forum posts\n",
    "        self.PrintUpdate('attempting to obtain Booter URLs')\n",
    "        self.PrintDivider()\n",
    "        # start crawilng for each forum item\n",
    "        counter = 0\n",
    "        for item in forum_items:\n",
    "            # parse html\n",
    "            response = self.Session.post(item.URL, headers=self.Header)\n",
    "            tree \t = html.fromstring(response.text)\n",
    "            url \t = ''\n",
    "            # check for URLs inside image tags\n",
    "            tree_image = tree.xpath('(//tbody)[1]//div[contains(@class,\"post_body\")]//a[.//img and not(contains(@href, \"hackforums.net\")) and not(contains(@href, \".jpg\") or contains(@href, \".png\") or contains(@href, \".jpeg\") or contains(@href, \"gif\"))]/@href')\n",
    "            if tree_image:\n",
    "                url = tree_image[0]\n",
    "            else:\n",
    "                # otherwise check for URL in the post's content\n",
    "                tree_links = tree.xpath('(//tbody)[1]//div[contains(@class,\"post_body\")]//a[not(@onclick) and not(contains(@href, \"hackforums.net\")) and not(contains(@href, \".jpg\") or contains(@href, \".png\") or contains(@href, \".jpeg\") or contains(@href, \".gif\"))]/@href')\n",
    "                if tree_links:\n",
    "                    url = tree_links[0]\n",
    "\n",
    "            # add found url to list\n",
    "            if url != '':\n",
    "                self.AddToList(BooterURL(url), item.URL)\n",
    "\n",
    "            # print a divider line every 10 results (to keep things organized)\n",
    "            counter = counter + 1\n",
    "            if counter % 10 == 0:        \n",
    "                self.PrintDivider()\n",
    "\n",
    "        # finished, print results\n",
    "        self.PrintDivider()\n",
    "        self.PrintUpdate('DONE; Resolved: ' + str(len(self.URLs)) + ' Booter URLs')\n",
    "        self.PrintDivider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"X\"><h1><a href=\"#TOC\">!!! Instantiating the Crawlers and collecting the list of URL potentially related to Booters websites.</a></h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "results_google \t   = []\n",
    "results_hackforums = []\n",
    "results_youtube\t   = []\n",
    "\n",
    "try:\n",
    "\t###############################################################################\n",
    "\t## GOOGLE V2                                                                 ##\n",
    "\t## ############################################################################\n",
    "\tcrawler = Crawler_Google2(1)\n",
    "\tcrawler.Crawl(100) # up to ~500 results (more is not possible by Google)\n",
    "\tresults_google = crawler.Finish('potentialBooters_google.txt')\n",
    "\n",
    "\tprint()\n",
    "\tprint()\n",
    "\tprint()\n",
    "\n",
    "\t###############################################################################\n",
    "\t## YOUTUBE                                                                   ##\n",
    "\t## ############################################################################\n",
    "\tcrawler = Crawler_Youtube(1)\n",
    "\tcrawler.Crawl(100) # up to ~500 results (similar limit to Google)\n",
    "\tresults_youtube = crawler.Finish('potentialBooters_youtube.txt')\n",
    "\n",
    "\tprint()\n",
    "\tprint()\n",
    "\tprint()\n",
    "\n",
    "\t###############################################################################\n",
    "\t## HACKFORUMS                                                                ##\n",
    "\t###############################################################################\n",
    "\tcrawler = Crawler_Hackforums(1) # sleep level of 1\n",
    "\tif crawler.Login():\t# from time to time it might give a 5 sec check browser period; if so, simply manually solve this by visiting hackforums.net\n",
    "\t\tprint('login succesfull')\n",
    "\t\tcrawler.Crawl(datetime.datetime(2015, 3, 1)) # crawl up to may 15th\n",
    "\t\tresults_hackforums = crawler.Finish('potentialBooters_hackforums.txt')\n",
    "\n",
    "\tprint()\n",
    "\tprint()\n",
    "\tprint()\n",
    "except Exception as ex:\n",
    "\tprint('GLOBAL EXCEPTION: ' + str(ex))\n",
    "\n",
    "###############################################################################\n",
    "## MERGE CRAWL RESULTS                                                       ##\n",
    "###############################################################################\n",
    "# final_results = []\n",
    "\n",
    "# def IsDuplicate(booterURL):\n",
    "# \tfor i in range(0, len(final_results)):\n",
    "# \t\tif booterURL.UniqueName() == final_results[i].UniqueName():\n",
    "# \t\t\treturn True\n",
    "# \treturn False\t\t\n",
    "\n",
    "# def AddResults(sub_results):\n",
    "# \tfor booterURL in sub_results:\n",
    "# \t\t# we make the assumption the uniqueness of a Booter url is based on its\n",
    "# \t\t# hostname only; thus we only have TYPE 1 URLs (prove this!)\n",
    "# \t\tif not IsDuplicate(booterURL):\n",
    "# \t\t\tfinal_results.append(booterURL)\n",
    "# \t\telse:\n",
    "# \t\t\tprint('duplicate:' + booterURL.Full_URL)\n",
    "\n",
    "# AddResults(results_google)\n",
    "# AddResults(results_youtube)\n",
    "# AddResults(results_hackforums)\n",
    "\n",
    "# crawler.PrintDivider();\n",
    "# crawler.PrintUpdate('completed crawling procedures; saving output to \\'crawler_output.txt\\'')\n",
    "# with open('crawler_output.txt', 'w') as f:\n",
    "\t# for booterURL in final_results:\n",
    "\t\t# f.write(booterURL.UniqueName() + '\\n')\n",
    "\t\t# json.dump(vars(booterURL), f)\n",
    "\t\t# f.write('\\n')\n",
    "\n",
    "###############################################################################\n",
    "## SCRAPE AND GENERATE SCORES                                                ##\n",
    "###############################################################################\n",
    "crawler.PrintDivider();\n",
    "crawler.PrintUpdate('INITIATING SCRAPING PROCEDURES;')\n",
    "crawler.PrintDivider();\n",
    "\n",
    "# query all to-scrape URLs\n",
    "# from_date    = datetime.datetime(2015, 8, 1).strftime('%Y-%m-%d %H:%M:%S') # test_scores\n",
    "# from_date    = datetime.datetime(2015, 8, 19).strftime('%Y-%m-%d %H:%M:%S') #test_scores2\n",
    "from_date    = datetime.datetime(2015, 8, 20, 13, 30).strftime('%Y-%m-%d %H:%M:%S') #test_scores3\n",
    "delay_period = 7\n",
    "\n",
    "for url in Select('SELECT fullURL FROM urls WHERE status != \\'off\\' AND timeUpdate >= \\'' + str(from_date) + '\\''):\n",
    "\tdelay = delay_period + random.randint(0,14) # add a slight randomness to delay_period as to divide workload\n",
    "\tdelay = 1\n",
    "\tcrawler.Scrape(BooterURL(url[0]), delay)\n",
    "\n",
    "crawler.PrintDivider();\n",
    "crawler.PrintUpdate('DONE;')\n",
    "crawler.PrintDivider();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
